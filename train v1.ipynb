{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Convolution2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Convolution2D, MaxPooling2D, UpSampling2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback, ModelCheckpoint\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "from lib.preprocess_modules import load_pkl, save_pkl\n",
    "from lib.dl_modules import *\n",
    "#from sklearn.preprocessing import normalize\n",
    "from config import *\n",
    "from pathlib import Path\n",
    "from numpy.random import shuffle, seed\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Generate final dataset from separate files.\n",
    "# X: array of n_files*64*64\n",
    "# y: array of n_files*16*16\n",
    "# Save to pkl files.\n",
    "# '''\n",
    "# n_files=0\n",
    "\n",
    "# for fname in os.listdir(foils_pkl_path):\n",
    "#     if fname[:2].lower() in ['ag', 'cl', 'mh', 'hq', 'hd', 'hn', 'ht' ]:   \n",
    "#         n_files +=1\n",
    "\n",
    "\n",
    "# X = np.zeros((n_files, 16, 32, 6))\n",
    "# y = np.zeros((n_files, 128))\n",
    "\n",
    "# sample = 0\n",
    "# for fname in os.listdir(foils_pkl_path):\n",
    "#     if fname[:2].lower() in ['ag', 'cl', 'mh', 'hq', 'hd', 'hn', 'ht' ]:\n",
    "#         foil = load_pkl(Path(foils_pkl_path, fname))\n",
    "#         x_layer=0\n",
    "#         for layer_n in [0,1,2,3,6,7]:\n",
    "# #         for layer_n in range(X.shape[3]):\n",
    "#             X[sample, :, :, x_layer] = foil['X'][layer_n, ...]\n",
    "#             x_layer+=1\n",
    "# #             if layer_n==6: break\n",
    "# #                 X[..., layer_n] = X[..., layer_n]/re_max\n",
    "# #             if layer_n==7:\n",
    "# #                 X[..., layer_n] = X[..., layer_n]/alfa_max\n",
    "#         y[sample, :] = foil['y'].y.reshape(128)\n",
    "#         sample+=1\n",
    "#         if sample>n_files-1:\n",
    "           \n",
    "#             break\n",
    "    \n",
    "# assert np.sum(np.isnan(X))==0, \"NaNs in X\"\n",
    "# assert np.sum(np.isnan(y))==0, \"NaNs in y\"\n",
    "\n",
    "# save_pkl(X, Path(foils_pkl_path, \"X.pkl\"))\n",
    "# save_pkl(y, Path(foils_pkl_path, \"y.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally foils in arrays: 64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Generate final dataset from separate files.\n",
    "X: array of n_files*\n",
    "y: array of n_files*\n",
    "Save to pkl files.\n",
    "'''\n",
    "n_X_layers = 7\n",
    "n_files=64\n",
    "X = np.zeros((n_files, 16, 32, n_X_layers))\n",
    "y = np.zeros((n_files, 128))\n",
    "\n",
    "sample = 0\n",
    "\n",
    "for fname in os.listdir(foils_pkl_path): # сперва нужные профили\n",
    "    if sample>n_files-1:break\n",
    "    if fname[:2].lower() in ['ag', 'cl', 'mh', 'hq', 'hd', 'hn', 'ht' ]:\n",
    "        foil = load_pkl(Path(foils_pkl_path, fname))\n",
    "        for layer_n in range(X.shape[3]):\n",
    "            X[sample, :, :, layer_n] = foil['X'][layer_n, ...]\n",
    "        y[sample, :] = foil['y'].y.reshape(128)\n",
    "        sample+=1\n",
    "\n",
    "for fname in os.listdir(foils_pkl_path): # потом все оставшиеся\n",
    "    if sample>n_files-1:break\n",
    "    if fname[:2].lower() not in ['ag', 'cl', 'mh', 'hq', 'hd', 'hn', 'ht' ]:\n",
    "        foil = load_pkl(Path(foils_pkl_path, fname))\n",
    "        for layer_n in range(X.shape[3]):\n",
    "            X[sample, :, :, layer_n] = foil['X'][layer_n, ...]\n",
    "        y[sample, :] = foil['y'].y.reshape(128)\n",
    "        sample+=1\n",
    "    \n",
    "assert np.sum(np.isnan(X))==0, \"NaNs in X\"\n",
    "assert np.sum(np.isnan(y))==0, \"NaNs in y\"\n",
    "\n",
    "save_pkl(X, Path(foils_pkl_path, \"X.pkl\"))\n",
    "save_pkl(y, Path(foils_pkl_path, \"y.pkl\"))\n",
    "\n",
    "print(\"Totally foils in arrays: %i\" % (sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 16, 32, 7), (64, 128))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = load_pkl(Path(foils_pkl_path, 'X.pkl'))\n",
    "y = load_pkl(Path(foils_pkl_path, 'y.pkl'))\n",
    "# X = X.reshape(X.shape[0],16,16,16)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(X))\n",
    "seed(42)\n",
    "shuffle(all_indices)\n",
    "\n",
    "n_indices = len(X)\n",
    "train_part = int(n_indices*train_percentage)\n",
    "val_part = int(n_indices*val_percentage)\n",
    "train_part, val_part\n",
    "train_indices = all_indices[:train_part]\n",
    "val_indices = all_indices[train_part:val_part]\n",
    "test_indices = all_indices[val_part:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net compiled\n",
      "Input shape: (None, 16, 32, 4)\n",
      "Output shape: (None, 128)\n"
     ]
    }
   ],
   "source": [
    "def define_unet_9(input_shape=(16, 32, 4), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   # contracting path only\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    bn1 = BatchNormalization()(inputs)\n",
    "       \n",
    "    conv1 = Convolution2D(4, (2,2), activation='tanh', padding='same')(bn1)\n",
    "    conv1 = Convolution2D(4, (2,2), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    flat = Flatten()(pool4)\n",
    "    \n",
    "    dense = Dense(2048, activation='tanh')(flat)\n",
    "    dense = Dense(128, activation='tanh')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "#         model.layers[layer].kernel_initializer=1e-12\n",
    "#         model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_unet_10(input_shape=(16, 32, 4), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "  \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    bn1 = BatchNormalization()(inputs)\n",
    "    \n",
    "    conv1 = Convolution2D(16, (2,2), activation='tanh', padding='same')(bn1)\n",
    "    conv1 = Convolution2D(16, (2,2), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(conv5)\n",
    "\n",
    "    up6 = Concatenate()([Convolution2D(256, (2, 2), activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv5)), conv4])\n",
    "    conv6 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(up6)\n",
    "    conv6 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(conv6)\n",
    "\n",
    "    up7 = Concatenate()([Convolution2D(128, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv6)), conv3])\n",
    "    conv7 = Convolution2D(512, (2,2), activation='tanh', padding='same')(up7)\n",
    "    conv7 = Convolution2D(512, (2,2), activation='tanh', padding='same')(conv7)\n",
    "    \n",
    "    up8 = Concatenate()([Convolution2D(64, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv7)), conv2])\n",
    "    conv8 = Convolution2D(256, (2,2), activation='tanh', padding='same')(up8)\n",
    "    conv8 = Convolution2D(256, (2,2), activation='tanh', padding='same')(conv8)\n",
    "    \n",
    "    up9 = Concatenate()([Convolution2D(32, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv8)), conv1])\n",
    "    conv9 = Convolution2D(16, (2,2), activation='tanh', padding='same')(up9)\n",
    "    conv9 = Convolution2D(16, (2,2), activation='tanh', padding='same')(conv9)\n",
    "    \n",
    "    conv10 = Convolution2D(1, (1, 1), activation='tanh')(conv9)\n",
    "\n",
    "    flat = Flatten()(conv10)\n",
    "    \n",
    "    dense = Dense(2048, activation='tanh')(flat)\n",
    "    dense = Dense(128, activation='tanh')(dense)\n",
    "        \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "#         model.layers[layer].kernel_initializer=1e-12\n",
    "#         model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = define_unet_9(input_shape=(16,32,4), optimizer=Adam(1), lr=1e-4, reg=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     try:   \n",
    "#         print(layer.output_shape)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net compiled\n",
      "Input shape: (None, 16, 32, 7)\n",
      "Output shape: (None, 128)\n",
      "Epoch 1/10000\n",
      "48/48 [==============================] - 6s 121ms/step - loss: 4.8989e-04 - mse: 4.8989e-04 - val_loss: 6.0648e-04 - val_mse: 6.0648e-04 - lr: 1.0000e-05\n",
      "Epoch 2/10000\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 1.4313e-04 - mse: 1.4313e-04 - val_loss: 5.1534e-04 - val_mse: 5.1534e-04 - lr: 1.0000e-05\n",
      "Epoch 3/10000\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 1.1005e-04 - mse: 1.1005e-04 - val_loss: 3.6407e-04 - val_mse: 3.6407e-04 - lr: 1.0000e-05\n",
      "Epoch 4/10000\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 8.8124e-05 - mse: 8.8124e-05 - val_loss: 3.5688e-04 - val_mse: 3.5688e-04 - lr: 1.0000e-05\n",
      "Epoch 5/10000\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 7.6065e-05 - mse: 7.6065e-05 - val_loss: 2.8525e-04 - val_mse: 2.8525e-04 - lr: 1.0000e-05\n",
      "Epoch 6/10000\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 6.5755e-05 - mse: 6.5755e-05 - val_loss: 2.4941e-04 - val_mse: 2.4941e-04 - lr: 1.0000e-05\n",
      "Epoch 7/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 5.1726e-05 - mse: 5.1726e-05 - val_loss: 2.8437e-04 - val_mse: 2.8437e-04 - lr: 1.0000e-05\n",
      "Epoch 8/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 6.1573e-05 - mse: 6.1573e-05 - val_loss: 2.4396e-04 - val_mse: 2.4396e-04 - lr: 1.0000e-05\n",
      "Epoch 9/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 5.2893e-05 - mse: 5.2893e-05 - val_loss: 2.1222e-04 - val_mse: 2.1222e-04 - lr: 1.0000e-05\n",
      "Epoch 10/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 4.7131e-05 - mse: 4.7131e-05 - val_loss: 2.5671e-04 - val_mse: 2.5671e-04 - lr: 1.0000e-05\n",
      "Epoch 11/10000\n",
      "48/48 [==============================] - 6s 117ms/step - loss: 4.1554e-05 - mse: 4.1554e-05 - val_loss: 2.3556e-04 - val_mse: 2.3556e-04 - lr: 1.0000e-05\n",
      "Epoch 12/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 3.8174e-05 - mse: 3.8174e-05 - val_loss: 2.2979e-04 - val_mse: 2.2979e-04 - lr: 1.0000e-05\n",
      "Epoch 13/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 4.0849e-05 - mse: 4.0849e-05 - val_loss: 2.4155e-04 - val_mse: 2.4155e-04 - lr: 1.0000e-05\n",
      "Epoch 14/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 3.0989e-05 - mse: 3.0989e-05 - val_loss: 2.6868e-04 - val_mse: 2.6868e-04 - lr: 1.0000e-05\n",
      "Epoch 15/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 3.3704e-05 - mse: 3.3704e-05 - val_loss: 2.6608e-04 - val_mse: 2.6608e-04 - lr: 1.0000e-05\n",
      "Epoch 16/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 3.2627e-05 - mse: 3.2627e-05 - val_loss: 2.4865e-04 - val_mse: 2.4865e-04 - lr: 1.0000e-05\n",
      "Epoch 17/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 2.9701e-05 - mse: 2.9701e-05 - val_loss: 2.5398e-04 - val_mse: 2.5398e-04 - lr: 1.0000e-05\n",
      "Epoch 18/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 2.4696e-05 - mse: 2.4696e-05 - val_loss: 2.6954e-04 - val_mse: 2.6954e-04 - lr: 1.0000e-05\n",
      "Epoch 19/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 2.6561e-05 - mse: 2.6561e-05 - val_loss: 2.5141e-04 - val_mse: 2.5141e-04 - lr: 1.0000e-05\n",
      "Epoch 20/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 2.4081e-05 - mse: 2.4081e-05 - val_loss: 2.8927e-04 - val_mse: 2.8927e-04 - lr: 1.0000e-05\n",
      "Epoch 21/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.3987e-05 - mse: 2.3987e-05\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 2.3987e-05 - mse: 2.3987e-05 - val_loss: 3.0442e-04 - val_mse: 3.0442e-04 - lr: 1.0000e-05\n",
      "Epoch 22/10000\n",
      "48/48 [==============================] - 6s 117ms/step - loss: 1.8929e-05 - mse: 1.8929e-05 - val_loss: 2.6341e-04 - val_mse: 2.6341e-04 - lr: 2.0000e-06\n",
      "Epoch 23/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.6021e-05 - mse: 1.6021e-05 - val_loss: 2.7966e-04 - val_mse: 2.7966e-04 - lr: 2.0000e-06\n",
      "Epoch 24/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.7061e-05 - mse: 1.7061e-05 - val_loss: 2.7006e-04 - val_mse: 2.7006e-04 - lr: 2.0000e-06\n",
      "Epoch 25/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.6114e-05 - mse: 1.6114e-05 - val_loss: 2.6810e-04 - val_mse: 2.6810e-04 - lr: 2.0000e-06\n",
      "Epoch 26/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.5755e-05 - mse: 1.5755e-05 - val_loss: 2.8011e-04 - val_mse: 2.8011e-04 - lr: 2.0000e-06\n",
      "Epoch 27/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.5871e-05 - mse: 1.5871e-05 - val_loss: 2.6702e-04 - val_mse: 2.6702e-04 - lr: 2.0000e-06\n",
      "Epoch 28/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.5694e-05 - mse: 1.5694e-05 - val_loss: 2.6683e-04 - val_mse: 2.6683e-04 - lr: 2.0000e-06\n",
      "Epoch 29/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.5338e-05 - mse: 1.5338e-05 - val_loss: 2.7508e-04 - val_mse: 2.7508e-04 - lr: 2.0000e-06\n",
      "Epoch 30/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.4524e-05 - mse: 1.4524e-05 - val_loss: 2.7624e-04 - val_mse: 2.7624e-04 - lr: 2.0000e-06\n",
      "Epoch 31/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.5304e-05 - mse: 1.5304e-05 - val_loss: 2.6833e-04 - val_mse: 2.6833e-04 - lr: 2.0000e-06\n",
      "Epoch 32/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.4737e-05 - mse: 1.4737e-05 - val_loss: 2.7564e-04 - val_mse: 2.7564e-04 - lr: 2.0000e-06\n",
      "Epoch 33/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.4512e-05 - mse: 1.4512e-05 - val_loss: 2.8056e-04 - val_mse: 2.8056e-04 - lr: 2.0000e-06\n",
      "Epoch 34/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.4821e-05 - mse: 1.4821e-05 - val_loss: 2.7326e-04 - val_mse: 2.7326e-04 - lr: 2.0000e-06\n",
      "Epoch 35/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.4340e-05 - mse: 1.4340e-05 - val_loss: 2.7593e-04 - val_mse: 2.7593e-04 - lr: 2.0000e-06\n",
      "Epoch 36/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.4352e-05 - mse: 1.4352e-05 - val_loss: 2.7944e-04 - val_mse: 2.7944e-04 - lr: 2.0000e-06\n",
      "Epoch 37/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.4043e-05 - mse: 1.4043e-05 - val_loss: 2.8133e-04 - val_mse: 2.8133e-04 - lr: 2.0000e-06\n",
      "Epoch 38/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.3310e-05 - mse: 1.3310e-05 - val_loss: 2.8422e-04 - val_mse: 2.8422e-04 - lr: 2.0000e-06\n",
      "Epoch 39/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.3222e-05 - mse: 1.3222e-05 - val_loss: 2.6957e-04 - val_mse: 2.6957e-04 - lr: 2.0000e-06\n",
      "Epoch 40/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.3775e-05 - mse: 1.3775e-05\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.999999989900971e-07.\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.3775e-05 - mse: 1.3775e-05 - val_loss: 2.6745e-04 - val_mse: 2.6745e-04 - lr: 2.0000e-06\n",
      "Epoch 41/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.2459e-05 - mse: 1.2459e-05 - val_loss: 2.7355e-04 - val_mse: 2.7355e-04 - lr: 4.0000e-07\n",
      "Epoch 42/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.1859e-05 - mse: 1.1859e-05 - val_loss: 2.7639e-04 - val_mse: 2.7639e-04 - lr: 4.0000e-07\n",
      "Epoch 43/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.1992e-05 - mse: 1.1992e-05 - val_loss: 2.7536e-04 - val_mse: 2.7536e-04 - lr: 4.0000e-07\n",
      "Epoch 44/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.2043e-05 - mse: 1.2043e-05 - val_loss: 2.7608e-04 - val_mse: 2.7608e-04 - lr: 4.0000e-07\n",
      "Epoch 45/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.1946e-05 - mse: 1.1946e-05 - val_loss: 2.7478e-04 - val_mse: 2.7478e-04 - lr: 4.0000e-07\n",
      "Epoch 46/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.2168e-05 - mse: 1.2168e-05 - val_loss: 2.7465e-04 - val_mse: 2.7465e-04 - lr: 4.0000e-07\n",
      "Epoch 47/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.1742e-05 - mse: 1.1742e-05 - val_loss: 2.7488e-04 - val_mse: 2.7488e-04 - lr: 4.0000e-07\n",
      "Epoch 48/10000\n",
      "48/48 [==============================] - 5s 113ms/step - loss: 1.1738e-05 - mse: 1.1738e-05 - val_loss: 2.7675e-04 - val_mse: 2.7675e-04 - lr: 4.0000e-07\n",
      "Epoch 49/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.1870e-05 - mse: 1.1870e-05 - val_loss: 2.7600e-04 - val_mse: 2.7600e-04 - lr: 4.0000e-07\n",
      "Epoch 50/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.1569e-05 - mse: 1.1569e-05 - val_loss: 2.7726e-04 - val_mse: 2.7726e-04 - lr: 4.0000e-07\n",
      "Epoch 51/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.1670e-05 - mse: 1.1670e-05 - val_loss: 2.7793e-04 - val_mse: 2.7793e-04 - lr: 4.0000e-07\n",
      "Epoch 52/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.1652e-05 - mse: 1.1652e-05 - val_loss: 2.8057e-04 - val_mse: 2.8057e-04 - lr: 4.0000e-07\n",
      "Epoch 53/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.1669e-05 - mse: 1.1669e-05 - val_loss: 2.7547e-04 - val_mse: 2.7547e-04 - lr: 4.0000e-07\n",
      "Epoch 54/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.1897e-05 - mse: 1.1897e-05 - val_loss: 2.7876e-04 - val_mse: 2.7876e-04 - lr: 4.0000e-07\n",
      "Epoch 55/10000\n",
      "48/48 [==============================] - 5s 115ms/step - loss: 1.1568e-05 - mse: 1.1568e-05 - val_loss: 2.7393e-04 - val_mse: 2.7393e-04 - lr: 4.0000e-07\n",
      "Epoch 56/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.1548e-05 - mse: 1.1548e-05 - val_loss: 2.7437e-04 - val_mse: 2.7437e-04 - lr: 4.0000e-07\n",
      "Epoch 57/10000\n",
      "48/48 [==============================] - 5s 113ms/step - loss: 1.1455e-05 - mse: 1.1455e-05 - val_loss: 2.7542e-04 - val_mse: 2.7542e-04 - lr: 4.0000e-07\n",
      "Epoch 58/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.1444e-05 - mse: 1.1444e-05 - val_loss: 2.7512e-04 - val_mse: 2.7512e-04 - lr: 4.0000e-07\n",
      "Epoch 59/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.1685e-05 - mse: 1.1685e-05\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 8.00000009348878e-08.\n",
      "48/48 [==============================] - 6s 121ms/step - loss: 1.1685e-05 - mse: 1.1685e-05 - val_loss: 2.7358e-04 - val_mse: 2.7358e-04 - lr: 4.0000e-07\n",
      "Epoch 60/10000\n",
      "48/48 [==============================] - 6s 121ms/step - loss: 1.1286e-05 - mse: 1.1286e-05 - val_loss: 2.7432e-04 - val_mse: 2.7432e-04 - lr: 8.0000e-08\n",
      "Epoch 61/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.1084e-05 - mse: 1.1084e-05 - val_loss: 2.7434e-04 - val_mse: 2.7434e-04 - lr: 8.0000e-08\n",
      "Epoch 62/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.1077e-05 - mse: 1.1077e-05 - val_loss: 2.7508e-04 - val_mse: 2.7508e-04 - lr: 8.0000e-08\n",
      "Epoch 63/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.1055e-05 - mse: 1.1055e-05 - val_loss: 2.7575e-04 - val_mse: 2.7575e-04 - lr: 8.0000e-08\n",
      "Epoch 64/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.1011e-05 - mse: 1.1011e-05 - val_loss: 2.7593e-04 - val_mse: 2.7593e-04 - lr: 8.0000e-08\n",
      "Epoch 65/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.1014e-05 - mse: 1.1014e-05 - val_loss: 2.7667e-04 - val_mse: 2.7667e-04 - lr: 8.0000e-08\n",
      "Epoch 66/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.1023e-05 - mse: 1.1023e-05 - val_loss: 2.7697e-04 - val_mse: 2.7697e-04 - lr: 8.0000e-08\n",
      "Epoch 67/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0979e-05 - mse: 1.0979e-05 - val_loss: 2.7595e-04 - val_mse: 2.7595e-04 - lr: 8.0000e-08\n",
      "Epoch 68/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0978e-05 - mse: 1.0978e-05 - val_loss: 2.7648e-04 - val_mse: 2.7648e-04 - lr: 8.0000e-08\n",
      "Epoch 69/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0984e-05 - mse: 1.0984e-05 - val_loss: 2.7769e-04 - val_mse: 2.7769e-04 - lr: 8.0000e-08\n",
      "Epoch 70/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0957e-05 - mse: 1.0957e-05 - val_loss: 2.7654e-04 - val_mse: 2.7654e-04 - lr: 8.0000e-08\n",
      "Epoch 71/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0985e-05 - mse: 1.0985e-05 - val_loss: 2.7746e-04 - val_mse: 2.7746e-04 - lr: 8.0000e-08\n",
      "Epoch 72/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.1030e-05 - mse: 1.1030e-05 - val_loss: 2.7563e-04 - val_mse: 2.7563e-04 - lr: 8.0000e-08\n",
      "Epoch 73/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0962e-05 - mse: 1.0962e-05 - val_loss: 2.7638e-04 - val_mse: 2.7638e-04 - lr: 8.0000e-08\n",
      "Epoch 74/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0924e-05 - mse: 1.0924e-05 - val_loss: 2.7619e-04 - val_mse: 2.7619e-04 - lr: 8.0000e-08\n",
      "Epoch 75/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0931e-05 - mse: 1.0931e-05 - val_loss: 2.7593e-04 - val_mse: 2.7593e-04 - lr: 8.0000e-08\n",
      "Epoch 76/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0953e-05 - mse: 1.0953e-05 - val_loss: 2.7661e-04 - val_mse: 2.7661e-04 - lr: 8.0000e-08\n",
      "Epoch 77/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0894e-05 - mse: 1.0894e-05 - val_loss: 2.7636e-04 - val_mse: 2.7636e-04 - lr: 8.0000e-08\n",
      "Epoch 78/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0971e-05 - mse: 1.0971e-05\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.5999999902760466e-08.\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0971e-05 - mse: 1.0971e-05 - val_loss: 2.7652e-04 - val_mse: 2.7652e-04 - lr: 8.0000e-08\n",
      "Epoch 79/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0830e-05 - mse: 1.0830e-05 - val_loss: 2.7577e-04 - val_mse: 2.7577e-04 - lr: 1.6000e-08\n",
      "Epoch 80/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0818e-05 - mse: 1.0818e-05 - val_loss: 2.7600e-04 - val_mse: 2.7600e-04 - lr: 1.6000e-08\n",
      "Epoch 81/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0813e-05 - mse: 1.0813e-05 - val_loss: 2.7631e-04 - val_mse: 2.7631e-04 - lr: 1.6000e-08\n",
      "Epoch 82/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0810e-05 - mse: 1.0810e-05 - val_loss: 2.7603e-04 - val_mse: 2.7603e-04 - lr: 1.6000e-08\n",
      "Epoch 83/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0819e-05 - mse: 1.0819e-05 - val_loss: 2.7628e-04 - val_mse: 2.7628e-04 - lr: 1.6000e-08\n",
      "Epoch 84/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0807e-05 - mse: 1.0807e-05 - val_loss: 2.7651e-04 - val_mse: 2.7651e-04 - lr: 1.6000e-08\n",
      "Epoch 85/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0826e-05 - mse: 1.0826e-05 - val_loss: 2.7637e-04 - val_mse: 2.7637e-04 - lr: 1.6000e-08\n",
      "Epoch 86/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0816e-05 - mse: 1.0816e-05 - val_loss: 2.7574e-04 - val_mse: 2.7574e-04 - lr: 1.6000e-08\n",
      "Epoch 87/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0816e-05 - mse: 1.0816e-05 - val_loss: 2.7585e-04 - val_mse: 2.7585e-04 - lr: 1.6000e-08\n",
      "Epoch 88/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0803e-05 - mse: 1.0803e-05 - val_loss: 2.7594e-04 - val_mse: 2.7594e-04 - lr: 1.6000e-08\n",
      "Epoch 89/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0805e-05 - mse: 1.0805e-05 - val_loss: 2.7604e-04 - val_mse: 2.7604e-04 - lr: 1.6000e-08\n",
      "Epoch 90/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0796e-05 - mse: 1.0796e-05 - val_loss: 2.7709e-04 - val_mse: 2.7709e-04 - lr: 1.6000e-08\n",
      "Epoch 91/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0804e-05 - mse: 1.0804e-05 - val_loss: 2.7690e-04 - val_mse: 2.7690e-04 - lr: 1.6000e-08\n",
      "Epoch 92/10000\n",
      "48/48 [==============================] - 6s 119ms/step - loss: 1.0801e-05 - mse: 1.0801e-05 - val_loss: 2.7654e-04 - val_mse: 2.7654e-04 - lr: 1.6000e-08\n",
      "Epoch 93/10000\n",
      "48/48 [==============================] - 6s 118ms/step - loss: 1.0811e-05 - mse: 1.0811e-05 - val_loss: 2.7644e-04 - val_mse: 2.7644e-04 - lr: 1.6000e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0812e-05 - mse: 1.0812e-05 - val_loss: 2.7645e-04 - val_mse: 2.7645e-04 - lr: 1.6000e-08\n",
      "Epoch 95/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0798e-05 - mse: 1.0798e-05 - val_loss: 2.7658e-04 - val_mse: 2.7658e-04 - lr: 1.6000e-08\n",
      "Epoch 96/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0795e-05 - mse: 1.0795e-05 - val_loss: 2.7649e-04 - val_mse: 2.7649e-04 - lr: 1.6000e-08\n",
      "Epoch 97/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0797e-05 - mse: 1.0797e-05\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 3.1999999094978194e-09.\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0797e-05 - mse: 1.0797e-05 - val_loss: 2.7589e-04 - val_mse: 2.7589e-04 - lr: 1.6000e-08\n",
      "Epoch 98/10000\n",
      "48/48 [==============================] - 5s 113ms/step - loss: 1.0772e-05 - mse: 1.0772e-05 - val_loss: 2.7613e-04 - val_mse: 2.7613e-04 - lr: 3.2000e-09\n",
      "Epoch 99/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0770e-05 - mse: 1.0770e-05 - val_loss: 2.7652e-04 - val_mse: 2.7652e-04 - lr: 3.2000e-09\n",
      "Epoch 100/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0769e-05 - mse: 1.0769e-05 - val_loss: 2.7688e-04 - val_mse: 2.7688e-04 - lr: 3.2000e-09\n",
      "Epoch 101/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0769e-05 - mse: 1.0769e-05 - val_loss: 2.7623e-04 - val_mse: 2.7623e-04 - lr: 3.2000e-09\n",
      "Epoch 102/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0774e-05 - mse: 1.0774e-05 - val_loss: 2.7666e-04 - val_mse: 2.7666e-04 - lr: 3.2000e-09\n",
      "Epoch 103/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0769e-05 - mse: 1.0769e-05 - val_loss: 2.7637e-04 - val_mse: 2.7637e-04 - lr: 3.2000e-09\n",
      "Epoch 104/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0771e-05 - mse: 1.0771e-05 - val_loss: 2.7697e-04 - val_mse: 2.7697e-04 - lr: 3.2000e-09\n",
      "Epoch 105/10000\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 1.0767e-05 - mse: 1.0767e-05 - val_loss: 2.7725e-04 - val_mse: 2.7725e-04 - lr: 3.2000e-09\n",
      "Epoch 106/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0767e-05 - mse: 1.0767e-05 - val_loss: 2.7652e-04 - val_mse: 2.7652e-04 - lr: 3.2000e-09\n",
      "Epoch 107/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0768e-05 - mse: 1.0768e-05 - val_loss: 2.7693e-04 - val_mse: 2.7693e-04 - lr: 3.2000e-09\n",
      "Epoch 108/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0767e-05 - mse: 1.0767e-05 - val_loss: 2.7652e-04 - val_mse: 2.7652e-04 - lr: 3.2000e-09\n",
      "Epoch 109/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0767e-05 - mse: 1.0767e-05 - val_loss: 2.7645e-04 - val_mse: 2.7645e-04 - lr: 3.2000e-09\n",
      "Epoch 110/10000\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 1.0766e-05 - mse: 1.0766e-05 - val_loss: 2.7641e-04 - val_mse: 2.7641e-04 - lr: 3.2000e-09\n",
      "Epoch 111/10000\n",
      "48/48 [==============================] - 5s 115ms/step - loss: 1.0770e-05 - mse: 1.0770e-05 - val_loss: 2.7650e-04 - val_mse: 2.7650e-04 - lr: 3.2000e-09\n",
      "Epoch 112/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0766e-05 - mse: 1.0766e-05 - val_loss: 2.7620e-04 - val_mse: 2.7620e-04 - lr: 3.2000e-09\n",
      "Epoch 113/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0767e-05 - mse: 1.0767e-05 - val_loss: 2.7643e-04 - val_mse: 2.7643e-04 - lr: 3.2000e-09\n",
      "Epoch 114/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0767e-05 - mse: 1.0767e-05 - val_loss: 2.7676e-04 - val_mse: 2.7676e-04 - lr: 3.2000e-09\n",
      "Epoch 115/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0766e-05 - mse: 1.0766e-05 - val_loss: 2.7643e-04 - val_mse: 2.7643e-04 - lr: 3.2000e-09\n",
      "Epoch 116/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0769e-05 - mse: 1.0769e-05\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 6.399999641359955e-10.\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0769e-05 - mse: 1.0769e-05 - val_loss: 2.7655e-04 - val_mse: 2.7655e-04 - lr: 3.2000e-09\n",
      "Epoch 117/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7642e-04 - val_mse: 2.7642e-04 - lr: 6.4000e-10\n",
      "Epoch 118/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7643e-04 - val_mse: 2.7643e-04 - lr: 6.4000e-10\n",
      "Epoch 119/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7650e-04 - val_mse: 2.7650e-04 - lr: 6.4000e-10\n",
      "Epoch 120/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7666e-04 - val_mse: 2.7666e-04 - lr: 6.4000e-10\n",
      "Epoch 121/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7653e-04 - val_mse: 2.7653e-04 - lr: 6.4000e-10\n",
      "Epoch 122/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7663e-04 - val_mse: 2.7663e-04 - lr: 6.4000e-10\n",
      "Epoch 123/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7632e-04 - val_mse: 2.7632e-04 - lr: 6.4000e-10\n",
      "Epoch 124/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7663e-04 - val_mse: 2.7663e-04 - lr: 6.4000e-10\n",
      "Epoch 125/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7688e-04 - val_mse: 2.7688e-04 - lr: 6.4000e-10\n",
      "Epoch 126/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7610e-04 - val_mse: 2.7610e-04 - lr: 6.4000e-10\n",
      "Epoch 127/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7586e-04 - val_mse: 2.7586e-04 - lr: 6.4000e-10\n",
      "Epoch 128/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7580e-04 - val_mse: 2.7580e-04 - lr: 6.4000e-10\n",
      "Epoch 129/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7554e-04 - val_mse: 2.7554e-04 - lr: 6.4000e-10\n",
      "Epoch 130/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7647e-04 - val_mse: 2.7647e-04 - lr: 6.4000e-10\n",
      "Epoch 131/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7654e-04 - val_mse: 2.7654e-04 - lr: 6.4000e-10\n",
      "Epoch 132/10000\n",
      "48/48 [==============================] - 5s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7641e-04 - val_mse: 2.7641e-04 - lr: 6.4000e-10\n",
      "Epoch 133/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7655e-04 - val_mse: 2.7655e-04 - lr: 6.4000e-10\n",
      "Epoch 134/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7716e-04 - val_mse: 2.7716e-04 - lr: 6.4000e-10\n",
      "Epoch 135/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0762e-05 - mse: 1.0762e-05\n",
      "Epoch 00135: ReduceLROnPlateau reducing learning rate to 1.27999988386307e-10.\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7695e-04 - val_mse: 2.7695e-04 - lr: 6.4000e-10\n",
      "Epoch 136/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7683e-04 - val_mse: 2.7683e-04 - lr: 1.2800e-10\n",
      "Epoch 137/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7673e-04 - val_mse: 2.7673e-04 - lr: 1.2800e-10\n",
      "Epoch 138/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7674e-04 - val_mse: 2.7674e-04 - lr: 1.2800e-10\n",
      "Epoch 139/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7647e-04 - val_mse: 2.7647e-04 - lr: 1.2800e-10\n",
      "Epoch 140/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7644e-04 - val_mse: 2.7644e-04 - lr: 1.2800e-10\n",
      "Epoch 141/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7691e-04 - val_mse: 2.7691e-04 - lr: 1.2800e-10\n",
      "Epoch 142/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7612e-04 - val_mse: 2.7612e-04 - lr: 1.2800e-10\n",
      "Epoch 143/10000\n",
      "48/48 [==============================] - 5s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7622e-04 - val_mse: 2.7622e-04 - lr: 1.2800e-10\n",
      "Epoch 144/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7641e-04 - val_mse: 2.7641e-04 - lr: 1.2800e-10\n",
      "Epoch 145/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7612e-04 - val_mse: 2.7612e-04 - lr: 1.2800e-10\n",
      "Epoch 146/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7641e-04 - val_mse: 2.7641e-04 - lr: 1.2800e-10\n",
      "Epoch 147/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7655e-04 - val_mse: 2.7655e-04 - lr: 1.2800e-10\n",
      "Epoch 148/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7630e-04 - val_mse: 2.7630e-04 - lr: 1.2800e-10\n",
      "Epoch 149/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7606e-04 - val_mse: 2.7606e-04 - lr: 1.2800e-10\n",
      "Epoch 150/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7614e-04 - val_mse: 2.7614e-04 - lr: 1.2800e-10\n",
      "Epoch 151/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7667e-04 - val_mse: 2.7667e-04 - lr: 1.2800e-10\n",
      "Epoch 152/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7660e-04 - val_mse: 2.7660e-04 - lr: 1.2800e-10\n",
      "Epoch 153/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7647e-04 - val_mse: 2.7647e-04 - lr: 1.2800e-10\n",
      "Epoch 154/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0762e-05 - mse: 1.0762e-05\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 2.559999712214989e-11.\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7641e-04 - val_mse: 2.7641e-04 - lr: 1.2800e-10\n",
      "Epoch 155/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7630e-04 - val_mse: 2.7630e-04 - lr: 2.5600e-11\n",
      "Epoch 156/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7680e-04 - val_mse: 2.7680e-04 - lr: 2.5600e-11\n",
      "Epoch 157/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7649e-04 - val_mse: 2.7649e-04 - lr: 2.5600e-11\n",
      "Epoch 158/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7693e-04 - val_mse: 2.7693e-04 - lr: 2.5600e-11\n",
      "Epoch 159/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7681e-04 - val_mse: 2.7681e-04 - lr: 2.5600e-11\n",
      "Epoch 160/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7718e-04 - val_mse: 2.7718e-04 - lr: 2.5600e-11\n",
      "Epoch 161/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7723e-04 - val_mse: 2.7723e-04 - lr: 2.5600e-11\n",
      "Epoch 162/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7710e-04 - val_mse: 2.7710e-04 - lr: 2.5600e-11\n",
      "Epoch 163/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7697e-04 - val_mse: 2.7697e-04 - lr: 2.5600e-11\n",
      "Epoch 164/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7667e-04 - val_mse: 2.7667e-04 - lr: 2.5600e-11\n",
      "Epoch 165/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7638e-04 - val_mse: 2.7638e-04 - lr: 2.5600e-11\n",
      "Epoch 166/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7676e-04 - val_mse: 2.7676e-04 - lr: 2.5600e-11\n",
      "Epoch 167/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7712e-04 - val_mse: 2.7712e-04 - lr: 2.5600e-11\n",
      "Epoch 168/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7666e-04 - val_mse: 2.7666e-04 - lr: 2.5600e-11\n",
      "Epoch 169/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7644e-04 - val_mse: 2.7644e-04 - lr: 2.5600e-11\n",
      "Epoch 170/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7647e-04 - val_mse: 2.7647e-04 - lr: 2.5600e-11\n",
      "Epoch 171/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7695e-04 - val_mse: 2.7695e-04 - lr: 2.5600e-11\n",
      "Epoch 172/10000\n",
      "48/48 [==============================] - 6s 116ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7686e-04 - val_mse: 2.7686e-04 - lr: 2.5600e-11\n",
      "Epoch 173/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0762e-05 - mse: 1.0762e-05\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 5.119999563207856e-12.\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7593e-04 - val_mse: 2.7593e-04 - lr: 2.5600e-11\n",
      "Epoch 174/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7587e-04 - val_mse: 2.7587e-04 - lr: 5.1200e-12\n",
      "Epoch 175/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7661e-04 - val_mse: 2.7661e-04 - lr: 5.1200e-12\n",
      "Epoch 176/10000\n",
      "48/48 [==============================] - 5s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7672e-04 - val_mse: 2.7672e-04 - lr: 5.1200e-12\n",
      "Epoch 177/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7673e-04 - val_mse: 2.7673e-04 - lr: 5.1200e-12\n",
      "Epoch 178/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7637e-04 - val_mse: 2.7637e-04 - lr: 5.1200e-12\n",
      "Epoch 179/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7622e-04 - val_mse: 2.7622e-04 - lr: 5.1200e-12\n",
      "Epoch 180/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7615e-04 - val_mse: 2.7615e-04 - lr: 5.1200e-12\n",
      "Epoch 181/10000\n",
      "48/48 [==============================] - 5s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7638e-04 - val_mse: 2.7638e-04 - lr: 5.1200e-12\n",
      "Epoch 182/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7600e-04 - val_mse: 2.7600e-04 - lr: 5.1200e-12\n",
      "Epoch 183/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7633e-04 - val_mse: 2.7633e-04 - lr: 5.1200e-12\n",
      "Epoch 184/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7635e-04 - val_mse: 2.7635e-04 - lr: 5.1200e-12\n",
      "Epoch 185/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7677e-04 - val_mse: 2.7677e-04 - lr: 5.1200e-12\n",
      "Epoch 186/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7606e-04 - val_mse: 2.7606e-04 - lr: 5.1200e-12\n",
      "Epoch 187/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7669e-04 - val_mse: 2.7669e-04 - lr: 5.1200e-12\n",
      "Epoch 188/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7658e-04 - val_mse: 2.7658e-04 - lr: 5.1200e-12\n",
      "Epoch 189/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7635e-04 - val_mse: 2.7635e-04 - lr: 5.1200e-12\n",
      "Epoch 190/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7673e-04 - val_mse: 2.7673e-04 - lr: 5.1200e-12\n",
      "Epoch 191/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7679e-04 - val_mse: 2.7679e-04 - lr: 5.1200e-12\n",
      "Epoch 192/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0762e-05 - mse: 1.0762e-05\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1.0239998952943363e-12.\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7660e-04 - val_mse: 2.7660e-04 - lr: 5.1200e-12\n",
      "Epoch 193/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7644e-04 - val_mse: 2.7644e-04 - lr: 1.0240e-12\n",
      "Epoch 194/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7637e-04 - val_mse: 2.7637e-04 - lr: 1.0240e-12\n",
      "Epoch 195/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7647e-04 - val_mse: 2.7647e-04 - lr: 1.0240e-12\n",
      "Epoch 196/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7636e-04 - val_mse: 2.7636e-04 - lr: 1.0240e-12\n",
      "Epoch 197/10000\n",
      "48/48 [==============================] - 6s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7645e-04 - val_mse: 2.7645e-04 - lr: 1.0240e-12\n",
      "Epoch 198/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7647e-04 - val_mse: 2.7647e-04 - lr: 1.0240e-12\n",
      "Epoch 199/10000\n",
      "48/48 [==============================] - 5s 115ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7621e-04 - val_mse: 2.7621e-04 - lr: 1.0240e-12\n",
      "Epoch 200/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7652e-04 - val_mse: 2.7652e-04 - lr: 1.0240e-12\n",
      "Epoch 201/10000\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7658e-04 - val_mse: 2.7658e-04 - lr: 1.0240e-12\n",
      "Epoch 202/10000\n",
      "48/48 [==============================] - ETA: 0s - loss: 1.0762e-05 - mse: 1.0762e-05Restoring model weights from the end of the best epoch.\n",
      "48/48 [==============================] - 5s 114ms/step - loss: 1.0762e-05 - mse: 1.0762e-05 - val_loss: 2.7644e-04 - val_mse: 2.7644e-04 - lr: 1.0240e-12\n",
      "Epoch 00202: early stopping\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "gc.collect()\n",
    "\n",
    "# n_epochs=20\n",
    "# batch_size=16\n",
    "# lr=5e-5\n",
    "# reg=0.01\n",
    "\n",
    "n_epochs=10000\n",
    "batch_size=1\n",
    "lr=1e-5\n",
    "reg=0.001\n",
    "optimizer=Adam(learning_rate=lr)\n",
    "\n",
    "model = define_unet_9(input_shape=X[0].shape, optimizer=optimizer, lr=lr, reg=reg)\n",
    "\n",
    "# model.load_weights(str(Path(\"./weights\", 'initial_weights_300foils_04122020.h5')))\n",
    "\n",
    "# data generators\n",
    "train_generator = BatchGenerator(X, y, train_indices, batch_size=batch_size, Xdim=X[0].shape)\n",
    "val_generator   = BatchGenerator(X, y, val_indices, batch_size=batch_size, Xdim=X[0].shape)\n",
    "test_generator  = BatchGenerator(X, y, test_indices, batch_size=batch_size, Xdim=X[0].shape, shuffle=False)\n",
    "\n",
    "# callbacks\n",
    "early_stop = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True, verbose=1)\n",
    "lr_reduce  = ReduceLROnPlateau(monitor='loss', min_lr=0, cooldown=10, factor=0.2, patience=10, verbose=1, mode='min')\n",
    "# lr_reduce = CosineLR(min_lr=1e-12, max_lr=lr, steps_per_epoch=np.ceil(len(train_indices)/batch_size), lr_decay=0.9)\n",
    "# m_save = ModelCheckpoint(str(Path(results_path, 'weights', 'temp_weights.h5')), monitor='val_mse', \n",
    "#                          verbose=0, save_best_only=True, save_weights_only=True, mode='min', period=1)\n",
    "\n",
    "# train model\n",
    "history = model.fit_generator(generator=train_generator, validation_data=val_generator, \n",
    "                             epochs=n_epochs, callbacks=[early_stop, lr_reduce],#, m_save],                              \n",
    "                             verbose=1, workers=1, use_multiprocessing=False)\n",
    "\n",
    "# save history and weights\n",
    "# r_name = (str(datetime.now())[:16]).replace(':','-')\n",
    "# unet.save_weights(str(Path(results_path,'weights', r_name+'.h5')))\n",
    "# save_pkl(history.history, Path(results_path,'weights', r_name+'.pkl'))\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07256468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAFlCAYAAAAterT5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAACUVElEQVR4nOzdd1hU19bA4d+ZoXdRsIK9d0XsvcReojEao8Zoqppqkpvkpn2pN9UkJqaZoknsGmtix96wV+wCFqqA9DLn+2NjbKAMTAFc7/PwADNnzlkIwpo9a6+l6bqOEEIIIYQQouAM9g5ACCGEEEKIkkaSaCGEEEIIIcwkSbQQQgghhBBmkiRaCCGEEEIIM0kSLYQQQgghhJkkiRZCCCGEEMJMDvYOoDDKlSunV6tWzd5hCCGEEEKIUm7Pnj2xuq773Xp7iUyiq1WrRmhoqL3DEEIIIYQQpZymaefzul3KOYQQQgghhDCTJNFCCCGEEEKYSZJoIYQQQgghzCRJtBBCCCGEEGaSJFoIIYQQQggzSRIthBBCCCGEmSSJFkIIIYQQwkySRAshhBBCCGEmSaKFEEIIIYQwkyTRQgghhBBCmEmSaCGEEEIIIcwkSbQQQuQlORqSLtk7CiGEEMWUJNFCCHGrzBSY0QvmjLR3JEIIIYopB3sHIITIlZEMV87lvp2F8g2hZjd7R1U8LHoCfAKh2+u2ud7699X34AqQEgvu5WxzXSGEECWGJNFC2JIpByJDIf40xJ+9njBfOQcpMTcf61oGXjwBDk72iLT4iNgNB+eAwQGaPQS+1a1/vR3fQkAbiNgBZzdCo6HWvaYQQogSR5JoIWwl+jgsnQSRu9XnmgG8qoBvNajbB8pUhzLVVJIYdxoWjofT66Fub3tGbX9bp4KLN2RnQMhHcP/31rtWdgYsmQheleGhOTC1KZzeIEm0EEKI20gSLYS15WSpRHDjx+DkAQO/hqrtwTsg/1Xm8o1g5UtwaP69nUTHnIDjK6DTFJXgbvsaOjwP/vWsc72NH0NsGIxaqF4JqN4RzoSAroOmWeeaQgghSiTZWCiENV06AD92hfXvQb1+MHEXtBgDZWveuUzD6AgNB0PYSlUrfa/a9iU4OEPwEyp5dvKADe9b51qXDsCWL6DpQ1C7h7qtRhdIjID4M9a5phBCiBJLVqKFsIasdNj0MWyZqjalPfg71B9w22E5Jp2ktCwS0rK4kppJYmoWCWmZJKRm4ZXVlqFZP7N07o/s9elFRraJjOwcMrNNZGSbct/nAGDQNIwGDU3TMGrqc03TMBqu3+fmZMTNyQEPZwfcnI24Ozng5mTM/dwBdycjni6OlPVwooybE0aDnVdeky7CgbnQcix4+Knb2k6EjR/BxX1QqbnlrpWTpco43MvBfTck6TW6qvdnQtQTHyGEECKXJNFCWFrEbvQlE9Fiw4irPYzQui8RHuPEhZNHuJiQRlRSukqaUzJJSs/O9zQarrRxLofXqSUsMjTA2dGIk9GAs6Mh970RZ6MBNMjOMWHSdXJ00HVdfWxSH+eY1FtqZg4pmdmkZGRj0u/8JWgalHFzoqy7E77uTpTzcKash/q4rIczFbxcqOTjQhUfN7xcHdCsUeqw41vQTdBu8vXb2k6EXd+rlf2HF1ruWlunwuVD6smOm+/128vWVHXrZ0Kg1XjLXU8IIUSJJ0m0EIVkMulcSEjjRNRVTkYnc+5SLB0iptM35S8u6768mvUKGw81hUOnAXB3MlLJx5UK3i5UK+eOj6sj3m5O+Lg6UsbdER9XJ7zdHNXnbk64OzvguCGUytu+5uCUlhZrs6brOhnZJlIysm9IrHNIycgmKT2L+JRMYpMziUvOID4lk7jkTI5dTiI+Ra2Q38rD2YHKPq5U8nGhchlXKvu45b53pXo5d3zdC9FdJO0KhP4CDYeozZbXuHhB++dg7VtwfjtUbVvof4d/RR9XtdANh9z+aoGmqZKO48tVZxWDsejXE0IIUSpIEi3EXdyYLJ+ISuZk9FVORiVzKjqZtCxVTlGJWOa4fEggl9jiM4jdtZ6hRzk/xni7UMnHlUo+rni5FGLFtvEDapX0yGIIfswiX4+mabg4GnFxNFLWzMdm5Zi4kpLJpcR0LiSkceFKmnqf+/He8AQS025OtMu4OVLDz4Ma5dzVez93avp5EOjrhpNDPtsyds+AzGRo/+zt9wU/rlap178Lj6wo2oY/U44q43DygD6f5H1MjS6w/3e4tB8qtyz8tYQQQpQqkkQLcYvE1Cz2RVxhb3gC+8KvsD8igas3lF2U93KmTnlPRgYHUru8Bw09U2i46nWMaakwYhkdqneig6WCqdAI/BvAoQUWS6KLwtFowN/LBX8vF5oG+OR5THJGNheupBF5JZWzsSmcjknhdEwyG8JimL8n8t/jjAaNQF83avp5UL+iJw0qelG/oheBnhqGnd9Bze5QscntF3Byg45T4O+XVAvAWt0L/wXtmA4XQuH+n67XXd+qRmf1/kyIJNFCCCH+JUm0uKflmHRORF1lb/gV9oUnsDf8CmdiUgAwaFCnvCcDmlaicWVv6pT3oJa/J96ujtdPkBwDvw6F1BgY/RcEtLJ8kI2Hwbr/gyvnoUxVy5/fwjycHahbwZO6FTxvuy8xLYuzsSmciUnmTG5yfTI6mfXHo/6t0x7ntJ63DDH8xCBcdpynfkUv6lXwxN35hl9XLceqdnfr31VTHQuzGh13WtVW1+mt/o3z/YL8VcvBMyHQ8UXzryOEEKJUkiRa3HPOx6Ww8UQMG8Ni2HEmjpRMVZLh6+5Ei0AfhraoQvNAH5pU8cHD+Q7/RVLjYdZgSIhQm9yskUADNMpNog8vKPFJnLerI80CfGh2yyp2elYOJ6KucvziFXqse4lTpjp8ebo8V48cBnJLk8u50yygDM0CfWge4EP9Ti9jXDZJ9ZGu39+8QEwmWPqMaiXY/4u7J+E1usCuHyAzVa2ECyGEuOdJEi1KvdTMbLafjlOJ84kYzselAhDo68bg5pUJqlaGFoFlCPR1K3jNcnoi/H4/xJ6Ah+ZCtfbW+wLKVFUjqA+V/CQ6Py6ORppU8aFJwnrIuIDv8FkcrH8fkVfSOHYpiWOXrnIwMoGQsGgW7lUlIe6OZVjtVAXHpW8QmtWMZoFlqejtUrDv4Z5f4PwWNfjGq9Ldj6/RBbZPU2PAa3Yr2hcrhBCiVJAkWpRKJ6OusiEsmo0nYth99gqZOSZcHY20rVmWR9tXp3MdP6qVcy/cyTOS4Y/h11ui2SKpajwMVk6By4dVnXRppOtqE2XZWlCvH5qmEeDrRoCvG70aVsg9RCfyShr7IlS9+uyTo5iS9D9WzfmGp00dKO/lTFA1X9pU96V1jbLU9ve4PamOPg5r3lSJcfPRBYstsC0YHFVJhyTRQgghkCRalCLn41JYduAiSw9c5ESUmvJXt7wnj7SvRqfafgRVK4OLYxFblGWlwZyRELkLhv0CdftYIPICaDgE/n5FjQEvrUn0mRA1NXDAV/m2krsxsR7YtBKY6mP6fgUfp66gZetH2ROZzK6z8aw4eAlQJTrB1XxpU8OX9uWzqXXsG7S9M8HZAwZ8WfBaamcPCAiG0xugZyG/voxkdR4hhBClgiTRokSLSkpn+cFLLD1wkQMRCQAEVS3D/w1qSM8G5ano7Wq5i2VnwNzRcHYzDPlejeW2FfdyqgvF4YXQ/S0w5NMaLi+6DslR4FnBevFZwtap4FEBmo4o+GMMBgzd/ovz7BGMcd3KmBGPoOs6EfFp7Dgbx84z8Rw6HU6jsC+pbPyHbC2bzV4DuNR0MsGZZaml6wUv4anRFTa8Bylx4G5mc8Ajf8HCCfDoP1AlyLzHCiGEKJYkiRYlzpWUTP4+fJmlBy6w82w8ug4NKnrxnz71GNC0EpV9LJg4X5OTBQsehVNr1Apm0wctf427afwALHpM1eVWbVfwx214HzZ/BuPX2C6Bu3wIji6B2r3UCu7dXNynVqJ7vAMOzuZdq05vqBykBqY0GYHm6EJgWTcCvQwMz1wCZz8FhyuEV+7Ln26j+fuiK+fXRMGaKCp5u9Cpjh+d6vjRvla5mzuv3KpGF5VEn90Ije4veHwmE4R8BKYs9WrC+DXmPQkSQghRLEkSLUoEXdfZfiaOmdvOs/ZYFNkmnRrl3HmmW20GNK1ELX8rvkxuyoHFT6ipdX0+hpaPWO9ad1K3Lzi6qZKOgibRx5bBptwhInt/s34SHX0MQj5UCTSoa1dtr6YM1u6Zf/nElqng7AVB48y/pqZB9zdh5kC1YTD4cTgwR8WRGKFqmLu/RWClZvwH+A8QeSWVzSdj2RgWw4pDl5izOwKDBs0Dy9Cpth+d6pSjSRUfjIYb4q3UXMV4JsS8JDpsJcQcgzp94MTf6vtnjydhQgghLErTdd3eMZgtKChIDw0NtXcYwgZSMrJZtO8CM7ed42R0Mj5ujgxrUYXBzSvTsJKX+RMACyo1HiJ3Q8QuOLMBLuxRq6QdnrPO9QpqwXg4vQ5ePAEOdxmnHRMGP3YDv7pQpjqcWAVTwsCpkBsq73itE7DxIzi8SE3/a/OUerJxbClsmwZJkarXcvvnVH238Ybn73GnYVoQtHsGer5T+Bh+GwBRR8DdXyWtlZqr79m1YSn5yM4xsT8igU253VsOXkhE18HHzZHOdfzo1aACnev6qXaHsx+CqMPw3MGCxaTr6nuQFg8Td8PPveBqFEwOtc73QQghhMVpmrZH1/XbVqEkiRbF0pmYZGZuP8/CPZFczcimUWUvxratxoCmlYq+OfBWJhPEhkHETojYrd7HnVT3aUao0BiaP1wsJgYS9g/MfhBGzoW6vfM/Lj1RJW/pifD4RrhyDn7tC4O/g2YjLRdP3GlVRnFoHji4QusnoN1kcPO9fkxOlmrPt3UqxBwHn0BoO1n9mzq5wbLnYP+fKjEtSt12xG6Y0RN8a0D3N6DB4EINYYlPyWTLKbVKvSEsmviUTJyMBtrXKstE9/UEHf0QntkPvtXvfrLT62HWEOg/Va2yh++An++DTi9Dt9fNjk0IIYTtSRItir0ck05IWDS/bT/PphMxOBo1+jauyJi21WgR6GP5Vefo47D6v2q1OSNR3ebqCwGt1eCUgNZqNbM4rRjmZMGndVSJwrAZeR9jMsGch1T99pilqoe1rsPXLcCrMjyyvOhxxJ+FTZ/CgdlgdFJPMNo/qzZA5sdkgpOrYMsX6omKW1kIehS2fqUS+wFfWiYu7ypqiIoF5Jh09py/wuojl1l19DJOV06xzvklvvecjNZqHL0aVLhzq8Rf+0PcKXj2wPVa7wWPqgExk3arJxRCCCGKNUmiRbGVlWNiwZ5IpoecJjw+lfJezoxqXZURwQH4e7pY56JpCfBDF7VS22BgbuLcWq1iWqtExFKWP69qfqeczLtlWshHqh64zyfQ+vHrt2/6RI25Lugqan7WvKUGjxgcIGi8Sp49y5t3jvPbVTJ9chWgweQ9ULZm4WOyAV3XOX4piSq/tuKAXouHr04EoE55D/o0qsjAZpWo6XfD9yN8pyrfuO8DaDvx+u0JETCtlWqP+MAvNv4qhBBCmEuSaFHsZOeYWLTvAl+vP0lEfBpNA3x4rGN17mtYAUejFbsXmEyq1/OptfDISghsbb1rWcP5bfBLH7j/R2gy/Ob7jq9UX1vTh2Dwtzc/IUiMhC8aQeeXoetrhbt2xC5VMtFoGPR6D7wqFv7rAFXDnBJ717rlYuWvpyFsJZETDrHmeCz/HL7MrnOqS0zDSl4MbFqJ/k0rUXnFWFVX//zh21/N2PABbPwfjPsHqra1z9chhBCiQCSJFsVGjklnyf4LfLXuJOfiUmlc2ZsXetahS10/620UvNHGT1SrsltXaksKkwm+bAL+9WHU/Ou3x55UddC+NVQ/Ysc8Wv3NGqKOe/Zg4dqs/TFcJYbPHbp3B4ccnKdaDT4eosp9gMuJ6Sw/eJFlBy9xICKBBto5Vjq/xr5aEwkY/BblPG5p25eZAl8HgYc/PLZBWt4JIUQxll8SLS3uhM3kmHRWHLrE1LUnOBOTQv2KXvw4Joge9f1tkzwDnFyr+iY3ebB4bBQsDIMBGg2FbV+rVVz3cpCepOqgjY5qFHleCTSozXwLHlW9jmt2Ne+6F/er8otu/713E2iA6rmr5mdC/k2iK3i7MKFjDSZ0rMH5uBQy54wlJcaVsYebkXJ0He1qlmVg00r0bVwRd2cHtTLd8x2VjB/4U31fhBBClCiy/CGszmTSWXnoEn2+3MQzs/fhYNCYPqoFKyZ3oGeD8rZLoK+cg4XjoXxD1S2huNc+30njB0DPgSOL1cr0X0+pThkP/Ao+Afk/rm4/cPGG/X+Yf83Nn4Kzt+rDfC/zLA/+DVQSnYeq+kVqx6zFvcOTzHuuN092rsG5uBReWnCQVu+v5eUFBwg9F4/eaBhUaQXr/g8yrtr2axBCCFFkshItrGpv+BXeXHKYwxeSqOnnztcjm9OvcUUMBhsnsFlpMG+M6lLx4CzVWq0kq9BIJXKHFkB6ghoEc9+HUL3TnR/n6KIS8H2/q82Vrj4Fu170MTW4pdPLKgm/19XoArtnqJ+rW1f9t0wFBxdoM5F6Hl7Uq+DFlF512XP+CvNDI1l+8CLzQiOp4efOpDqTuT9yDGz+HHq8ZY+vRAghRCHJSrSwiispmby66CD3f7uNmKsZfD68Kauf78yAppVsn0DrOqyYApcOwP0/qJrh0qDxMDUCfP370Hi4GnBSEM1GQXY6HF5Y8Gtt+vT6EBWhkuicDNX3+UYJ4XBwDrQcCx5+/96saRpB1Xz537Am7Hq9Bx8Pa0JZdyde2OrA4pyOZG39mk27dpOVY7Lt1yGEEKLQJIkWFmUy6czbHUG3z0KYFxrJYx2rs+7FLtzfosrNI5Rtac+vsP93tYp6pwElJU2jYep9hUaqx3JBy1MqNQf/hgUv6Yg9BUcWQavxNw9RuZdVba9a/N1a0rHta0BTA2fy4e7swPCgAOY/2Y71L3bmYquXydYNJC97jbYfruODlcc4HZNs1fCFEEIUnZRzCIs5dimJ//51mD3nrxBUtQzvDWlEvQpe9g0qcg/8/TLU6gFd/mPfWCytTFXVos+vnnnlKZoGzUfBqtfUwBn/enc+fssXaqBK20lFi7c0cfaAKsE3J9HJ0bB3JjQdoQa+FEANPw8mDuxEjueL9N34AUf9wvluSxY/bDpDcHVfRgYH0KdRRctP6RRCCFFkshItiiw5I5t3lx+l/9dbOBubwifDmjDvibb2T6BTYlUdtGcF1VPZUAoTkWrtwb2s+Y9r8qBaSd3/+52Pu3I+tzzhEdWOTVxXo4sqEUqNV59v/wZyMqHD82afytjhGfAOYIrpF7a90pmXe9clKimd5+ceoPUH63h76RHCLsvmQyGEKE4kiRaFpus6yw9epPtnIfy89SwjWgWw/sXOPBAUYPu651vlZKtWbqmxMHyWlCHcyr0c1OkNB+aqUeL52ToVNAO0e8ZmoZUYNboAumoXmHZFbTRsOKRwkxcdXVXLu6hD+J+az9NdarHhxS78OaE1ner48efOcO6buokh325l3u4IUjOzLf3VCCGEMJOUc4hCSUjN5JWFB1l1JIpGlb34fnQQzQJ87B3Wdev/TyU3g76FSs3sHU3x1GyU6upxcg3U63v7/UkXVRePZqPAu7Lt4yvuKrcAJ09V0hF7EjKvQocXCn++hvfDrh9h5cuQloCh3WTa1SpHu1rliE/JZNHeSGbvCuflhQf5v+VHGdSsEiODA2lUWbqlCCGEPcjEQmG2nWfieG7ufmKTM3jpvrqM71DDfpsGb5WeBCtehEPzoOU4GDDV3hEVXzlZ8HkDCAiGEXlsMvz7P7DrB3hmL5SpZvPwSoQ/R8DlQ5CVCgGt4aE5RTtfcgwsf049uakcpEa3+9X9925d1wk9f4XZu8JZcfASGdkmGlX2YkSrQAY1q4Sni6P514w9BRf3QYNB4OBUtPiFEKIUkrHfosiyc0x8vf4UX68/SaCvG1+PbEHjKsVoFSwyVA1TSYhQmwg7vlg666AtafV/Ycd0eOH4TS3ZSI6GqU2g0f0qkRN52/m92rgKMH4tBLQq+jl1XXVDWTFFjQfv+iq0nQzGm184TEzLYsn+C/y5M5zjl6/i6mhkQNOKjAgOpHmAT8GGGF06CDMHQVo8eAdCpxeh6UOFS6YzkmH/n+oJQNfXILCN+ecQQohiKL8k2iI10Zqm9dY0LUzTtFOapt3WAkFTvsq9/6CmaS1uuM9H07QFmqYd1zTtmKZpbS0Rk7CsCwlpjPxxB1+uO8mQ5lVY/kxHyybQuq7+oOcUotbTlAObP4Of71PT+8b9DZ1flgS6IJo9DKZstXJ/o+3fqF7SRSlPuBfU6KLeV+9kmQQaVPeURkNh4k6o0wvWvg0zeqpOKjfwdnVkTNtq/P1sR/6a2J5BzSqx/OAl7v92G72nbuaXrWdJTL1DvfvF/fDbAHB0gyHfqydRy56FaS1hz293rpW/0ZVzsOp1+Lw+/P2SejI7czCcWlfIfwAhhCgZirwSrWmaETgB9AQigd3ASF3Xj95wTF9gMtAXaA18qet669z7fgM267r+k6ZpToCbrusJd7qmrETb1t+HLvHKwoPkmHTeH9KYwc2tUB97YC4sfhw8K0KLMeqtIG3Cki7Cosfh3GZVU9r/i4JP4RPKj93U5L2ntqkELjUepjaGOvfBsJ/tHV3xputqEE39/uBf3zrnP7IYVk5Ro8G7/AfaPXvbqvQ1yRnZLDtwkdm7wjkYmYizg4G+jSsyolUAwdV9r69OX9gDs4aoMe6PLFPlOroOp9bChg/g4l7wqQqdXlIt+4y3lInoOpzfql7FCFsJaNBwMLR+SrVenHU/xIbB0BnQYKDl/12EEMKGrFbOkbty/Lau6/flfv4qgK7rH95wzPdAiK7rs3M/DwO6ACnAAaCGbkYgkkTbRlpmDu+uOMqfO8NpWsWbr0Y2p2pZd8tfSNfhuw6qrtS3pvpDrmlQ+z4IehRqdc97Vfn4SlgyUa2Y9vkYmj9c8IEj4rrQn2H58/DYBrVZbsMHsPF/8NR2KN/A3tEJULXSK1+Eo0vUsJxB3971e3PkYiJzdkXw174LXM3IpoafOyNaBfBghct4LxwBrmXgkeXgE3jzA3UdTq6GkA9VrXSZaiqZbjJCvWpxeAHs+A6iDoGrr2p/2GrCzZtP067AH8PhQigM+gaaPWTxfxIhhLAVaybRw4Deuq5PyP18NNBa1/VJNxyzHPhI1/UtuZ+vA14BsoEfgKNAU2AP8Kyu6yl3uqYk0dYXdvkqk2fv5URUMk90rsGLPevi5GCljoin16tVsUHfqET4yjn1cvK+WZASo2o1W46B5mPAs7xaNV39Buz+ESo2Vatd5WpbJ7Z7QXoifFpH/dt3f1OtQlfrmPdmQ2FfRxarjbMZV6HN06r14F36hKdmZrPi4CXm7I6A8B385vQxaU5lONtvLkFNGuffjlLX4cQqCPlA9cMuU03VPafGgn8DaP0kNBmu2vPlJSMZ5jykuuT0+QRaP160r10IIezEmkn0A8B9tyTRwbquT77hmBXAh7ck0S8DGrADaK/r+k5N074EknRdfyOP6zwOPA4QGBjY8vz580WKW+RvzdEonpm9D3dnBz4f3pROdfzu/qCimHU/RB2G5w6Bg/P127MzIWyFWik9u0kNB6nXT7UTiz6qJuh1f/Pmx4jCWThBrT4GPwGbPobHQ9SKpyh+UmLVtMmD81Q9c+vH1cbDuw3dObcV0+/DiDeW46HM1ziR5kWgrxsPtgrggZZV8Pdyyftxug5hf6uR5q4+Knmu3qlgr/pkpat+7WEroNsbarOvvFokhChhims5hw7s0HW9Wu7tHYH/6Lre707XlJVo6/l161neWX6UJpW9+XFsEP6e+fxhtZSoozC9rfoD22lK/sfFnoI9v8D+P1QyPeQ7NcpbWMbpDTBrMKBB7Z4war69IxJ3ExMGGz+Gwwvvnkyf3QR/Pqj2GYxdRrqLH6uOXGb2rnB2nInHaNDoVs+fh4ID6VTHz7ItK3Oy4K+n1ebV9s9Cj3ckkRZClCjWTKIdUBsLuwMXUBsLH9J1/cgNx/QDJnF9Y+FXuq4H5963GZig63qYpmlvA+66rr90p2tKEm15OSadD1YeY8aWs/RsUJ6vRjTH1ckG3S3+mqjaeT1/pGBTBXOy1AQ96bxhWSYTfNkEEiNg/BrVO1qUDHdLpk9vgNkj1Ya/sctuG99+NjaFObvDWbgnktjkTCp5u/BAUADDWwVQ2SefUg1zmUyqpjv0Z7XPoe9nYJCBuUKIksGqfaJzu29MBYzAz7quv69p2pMAuq5/p6kt4dOA3kAqME7X9dDcxzYDfgKcgDO591250/UkibastMwcnpu7j1VHohjXvhr/7dfANsNTrl6GLxqpjUn9PrX+9cSdHZynal/ve9/ekYjCuDWZDn4MKjaBxU9B2Vowdqka956PzGwT645FMXt3BJtPxgDQpY4fI4ID6VbPH0djEZNeXVft+rZOhcbDVf/xW7t+CCFEMSTDVkSeYpMzmPBbKAciE3ijXwMe7VDddhdf93+w+XOYvAfK1rTddYUozW5MptGhQmMYveTuNdM3iIhPZV5oBPNCI4hKysDP05kHWlZhRKtAAsu6FS2+zZ+p//v1B8DwWVLaIYQo9iSJFrc5HZPMI7/sIuZqBl+OaM59DSvY7uKZKWrkdLUO0gVCCGuICYNjSyFofMFKpfKQnWMiJCyG2bvC2RAWjUmHDrXKMSI4gJ4NyuPsUMiyqq1fwpo3YeA0aDG6cOcQQggbkSRa3GTnmTgen7UHR6PGT2Nb0SzAx7YB7PpRDZB4dJWMBxaiBLiUmMb80Ejm7o7gQkIavu5ODG1RmRHBgdT08zDvZCYT/NoXoo/BpNCbR84LIUQxI0m0+NeS/Rd4af5Bqvi68tu4YAJ8i/jyrLlMOfB1C3D3U5vY5OVcIUqMHJPOllOxzNkVzpqjUWSbdIKr+TKydQB9GlXExbGAq9MxYTC9PTS6H+7/wbpBCyFEEeSXRMv26HvMzO3neHbOfpoH+rDoqXa2T6ABjq9QA1XaTpIEWogSxmjQ6FzHj+kPt2T7q915pXc9oq+m8/zcAwS/v5a3lx7h+OWku5/Iry50eB4OzlUDl4QQooSRleh7yLzdEby88CA9G5Rn2kPNC1/PWFQzeqnOHM/sk1Z1QpQCJpPOjrNxzNkVwT+HL5OZY6JZgA8jgwPo36QS7s4OeT8wKx2mtwPdBE9vz3/6oRBC2JGsRN/jlh64yCuLDtKpjp99E+iIXRCxU40slgRaiFLBYNBoV7McX41szo7XuvPffvVJzsjmlYWHaP3BOl5bfIhDkYm3P9DRBfp/AVfOqo4iQghRgshK9D1gzdEonvp9Dy2qluG3ccG2GaKSn7mj4exGeP4oOJu5GUkIUWLous6e81eYvSuCFYcukp5lomElL0YGBzKoWSU8XW7oEb34KTXR8InNUL6B/YIWQog8yMbCe9SmEzFM+C2U+pW8+GNCazzye1nVFuLPqg2F7Z+FHm/bLw4hhE0lpmWxZP8FZu+K4NilJFwdjfRvUpERwYG0CPRBS42HaUFQrjaM+0emGQohipX8kmg7ZlTC2nadjefxWaHU9Pdg5rhgyyfQUUfhwJ/QYqz643c3O6aDZoTgJywbhxCiWPN2dWRM22qMblOVg5GJzNkdzpL9F5m/J5I65T0YGRzI8K7v4L5yMuz5BVqNt3fIQghxV7ISXUrtj0jg4Z92Ut7LmblPtKWch7NlL5AQAT/1gOTLgKbaVHV6Cfzr53182hX4vCE0GAhDvrNsLEKIEic5I5tlBy4yZ1c4ByITcXLQWO71CTWyTmKcvBvNq6K9QxRCCEA2Ft5Tjl1KYuzPu/B1d+KPCW0sn0CnJcAfD0BWKoxdBu2fgbB/4Ns2qub50sHbHxP6C2SlQNuJlo1FCFEieTg7MDI4kCWTOrDymY6MaBXI86ljyclMJ+Sr8fyw6TRxyRn2DlMIIfIlK9GlzKnoZB78fjtODgbmPdHW8n2gszPhj6Fwfjs8vBBqdFa3p8bDjm9h5/eQkQR1+kDnl6ByS/WYqY3Bvx6MWWLZeIQQpUZaZg6nF75Fo7CveSTzJbZqLejVoAIjggNoX7McBoP0lRdC2J5sLLwHRMSn8sB328k26cx7og01zB3Feze6DoufhINzYPB30Gzk7cekJahEese3kJ4ANbtDhUaw9UuVdNfqYdmYhBClS3YmfNeBrIwUPqs1k7kH4riSmkWArysPBgXwQFAA5b1c7B2lEOIeIkl0KZeUnsWgaVu5kprJnMfbUK+Cl+UvsuED2Pg/6PIadHnlzsemJ8Hun2D7NEiNA/8G8NQ2mVAohLi789vhl97QdhIZ3f+PVUeimL0znO1n4jAaNLrV82dkcACd6/hjlNVpIYSVSRJdiplMOk/8vocNx6P587E2BFf3tfxF9v0OSyZCs4dh0LSCJ8OZKWqsb6Xm6k0IIQpi6TPq985j6/793XE2NoW5uyNYsCeC2ORMKnq78EBQAA+2CqCyj0w7FEJYhyTRpdg3G07xyaow3uzfgEc7VLf8BU6vVxsJq3WEUfPB6Hj3xwghRFGkXYFpwZCeCEGPQofnwbM8AJnZJtYdi2L27gg2n4wBoHMdP0a0CqR7fX8cjbJnXghhOZJEl1KbTsQw9pddDGxaiakPNkOzdLnE5cPwc2/wCYRH/wEXK5SJCCFEXq6cV+PAD8wGo5PqH93+OfDw+/eQiPhU5odGMC80kstJ6fh5OjOsZRVGtAqgall3+8UuhCg1JIkuhSLiUxkwbQsVvFxY9HQ73JwsPEwl8YLqBQ0wYS14V7bs+YUQoiDiTsOmT1RpmIMLBD8G7Z4F97L/HpKdYyIkLIY5u8NZfzwakw7ta5VlRKtAetUvi3P8Sbh6CZy9wMX7+pujq+zVEELckSTRpUx6Vg7DvtvG+bhUlk3qQLVyFl5xSU+CX/qolaBH/4YKjS17fiGEMFfsKbW5+dB8cHSD1k9Au8ngdsM+kJwsYs8c4MCuDSSeCaV61inqG8JxITPvcxocbk6qXbzVVNV6fW3zNQkhij1JoksRXdd5ZeFB5oVG8tOYIHo0KG/pC8CfD8KptTBqnrSlE0IULzFhKpk+vAicPKDlWLWJ+dJ+iDoCOSph1p29SPSuT2hmICtiy3M+pxwtKjjSs4YLLfw1HLOuqprrG98u7gXvKvDEJvt+jUKIYiO/JNrCr/8LW5i9S9X/Te5Wy/IJNEDkbji5Cnr+nyTQQojix68uDPsZOr0EIR+pVpou3lCxKbR+Eio1g4rN0MpUx8dgoAfQ9GoGC/dGMmdXOD9tTcXLxYEhzSszIjiQ+hVv2Oux5QtY+zYkRqpkWggh8iEr0SXM/ogEhn+3nTY1y/LLI62s0yN1/iNwaj28cBScLTywRQghLC3jqlqRLkBts67r7DgTz5zd4fx96DKZOSaaBfgwMjiA/k0q4X71LEwLgj6fQOvHbRC8EKK4k3KOUiA2OYMBX2/BaNBYPrkDPm5Olr9IYiRMbQJtn4Ze71n+/EIIUUxcSclk0b4LzNkVzsnoZDycHRjQtBJvnx+Ls28VGLvU3iEKIYoBKeco4bJzTEz+cx/xKZksfKqddRJogF0/AjoEywqMEKJ0K+PuxPgO1Xm0fTX2hl/hz50RLN4XSaDegMcSV7J822HuC6qPq5PR3qEKIYoh6UhfQnyyOoztZ+J4f0hjGlX2ts5FMlNgz69Qr7/qCy2EEPcATdNoWdWXz4Y3ZedrPQhsOwwHctiw/A9af7CW/1t2lNMxyfYOUwhRzMhKdAmw62w83288w6jWgQxracWNLgfnQnoCtHnaetcQQohizNvVkX59BqIf9efNqufIcfZj1o5z/Lz1LO1qluXhNlXp2aC8TEUUQkgSXdxlZOfw2uJDVPZx5fV+9a13IZMJdkyHis0gsI31riOEEMWdwYBWry9lDy1g2su/EpPWkHmhEfy5M5yn/9iLv6czI1oFMKpNVcp7udg7WiGEnchT6WLuh41nOBWdzHuDG1l+IuGNzqyH2BPQ5imZ3iWEEHX7QWYynN2En6czE7vWYtPLXZkxNoiGlbz4esMpOvxvPS/M3c/hC4n2jlYIYQeyEl2MnY1N4esNp+jXpCJd6/lb92I7poNHeWg4xLrXEUKIkqB6J9U27/hyqN0TAKNBo3v98nSvX57zcSn8svUc80IjWLTvAm1q+DKhQw261fPHYI3Wo0KIYkdWoospXdd5ffEhnB0MvNW/gXUvFnNCTSdsNQEcnK17LSGEKAkcXaBWdwj7W5W73aJqWXfeHtiQ7a9257W+9QiPS2XCzFC6f76RmdvPkZqZbYeghRC2JEl0MbV43wW2nY7jld718Ld2zd3O78DoBC3HWfc6QghRktTrD8lRcGFPvod4uzryeKeabHy5K1+PbI6XqyNvLjlC2w/X89Hfx7mUmGa5eFLiLHcuIUSRSRJdDMWnZPLeimO0CPThoWArt5pLuwIHZkPj4eDhZ91rCSFESVK7JxgcVEnHXTgaDQxoWom/nm7Hwqfa0r5WWX7YdJpOH2/g1UWHiIhPLVosW76AT2tDxK6inUcIYTGSRBdDH648RlJaFh/c39j6tXV7Z0JWKrR50rrXEUKIksa1DFRtD2ErC/yQaz2nvx3Vko0vdeXBVgEs3BNJl09DmDL/AGdjU8yPI2IXrHsX9BzYPs38xwshrEKS6GJm++k45u+J5LFONahXwcu6F8vJhp0/QLWOUKGxda8lhBAlUb3+qnNR7EmzHxrg68Z7gxuz6eWujGlblWUHLtL9sxCenbOPk1FXC3aStARYMB68KkPLR+DYckiIMDsWIYTlSRJdjGRk5/D64kME+rrxTLfa1r/g8WWQFKna2gkhhLhdvb7q/fEVhT5FBW8X3hrQkM2vdGVCxxqsORpFr6mbmPjHXo5dSsr/gboOy5+DpAswbAZ0fBHQYfePhY5FCGE5kkQXI99uOM2Z2BTeG9wIVyej9S+4YzqUqQZ1elv/WkIIURJ5V4GKTYuURF/j7+nCa33rs+WVbjzdpSYbT8TQ58vNPDYzlCMX8+g1vW8WHFkM3V6HgGDwCVQr43t+g8xClIUIISxKkuhi4lR0MtNDTjOoWSU61bHBBr8LeyBiJwQ/AQYbJOxCCFFS1esPkbvhapRFTufr7sRL99Vj6yvdeK5HbXaeiaPfV1t4ds4+wuNyNyDGhMHKl6F6Z2j//PUHt3ka0hPgwByLxCKEKDxJoouBaz2hXRwN/LdfEXpCb/4M5o1VPZ/z6Gt6kx3fgZMnNH+48NcTQoh7Qd2+gA4n/rboab3dHHmuRx02565Mrzpyme6fh/Du4j1kz3sEnNxgyPdguOFPdWAbtTK+83tV7iGEsBtJoouB+Xsi2Xk2ntf61sfPs5DDTrIzYPMXcPQv+H0ofNUUNn2a98pJ0iU4skgl0C5W3rwohBAlXfmG4FPVIiUdefF2deTl3vXY+FJXhrUMoOrej3CIOcriqm9w1anczQdrGrR+CmLD4PR6q8QjhCgYSaLtLCk9iw9WHqNVtTIMDwoo/InObYHMqzB8Jgz7Wf3CX/8ufNEA5o5Wv2yvrU6HzgBTDrR+3DJfhBBClGaapko6zmyEjAJ21QDVASktocCHl/dy4cMG4YwxrmadzzCe3+dP509C+HnLWTKyc64f2Oh+cPdXg7KEEHYjSbSd/b7jPAmpWbzZv2HRekKf+AccXKF2L2g0FB5ZDpP2qM4b57bArCHwdXPY/DmE/gx1+4BvDct9IUIIUZrV6ws5GXBqXcGOz7gKvw2AT2rBwscgMv+ph/9KvABLJkKFJnSf9C1LJranXgVP/m/5Ubp/tpFFeyMxmXRwcIagR+Hkaog9VbSvSwhRaJJE21F6Vg4/bzlLpzp+NK7iXfgT6TqE/Q01u4Kj6/Xby9WCXu/Bi8dh6AzwqgLr3oHUOGgtw1WEEKLAAtqAq2/BBq+kJ8Gs+9Xm7YaD1e/nn7rBTz3g0ALIzrz9MaYcWPS4um/YL+DgTNMAH/6Y0JqZjwbj7erIC/MO0P/rLWw7FauSaKOTrEYLYUcO9g7gXjYvNILY5Eye7lKzaCeKOgKJEdDppbzvd3CGxsPUW8wJiDkO1TsV7ZpCCHEvMTqodqBhKyAnC4yOeR+XlgC/3w+XDsADv0CDQSqpPjBbbQZcOB48KkCr8Wp4ioe/etymT+H8Fhj8nVoAyaVpGp3q+NGhVjmWHrjIJ6vCeOinnXSv588XtQbitf9P6PZfcPWx9r+AEOIWshJtJ1k5Jr7feIaWVcvQurpv0U4WlrtjvCD9nv3qQIOBqsZPCCFEwdXrB+mJcH5r3venxsPMQXDpoNqf0mCQut3FC1o/AZNCYdQCqNAINrwPXzSExU/C3lmw8SNoPByajsjz1AaDxuDmlVn3Ymde6V2PXWfjefhwC8hKIXnnr9b5eoUQdyRJtJ0s3X+RCwlpTOxaE62oCW3YSqgcBJ7lLROcEEKI29XspvaeHM+jpCM1HmYOhOij8ODvKuG+lcEAtXvCwwtVQt3yETi2DJZOUpvB+39+1wUOF0cjT3WpSchLXWgW3JndprokbJjGt+vDSM/KueNjbxJ7SpWQCCEKTZJoOzCZdKZvPE29Cp50retftJNdvQwX90JdmToohBBW5eSm9p4cX3Fzj+aUWLWJMOYEjJhdsN/H5WpD30/ghaMw4Eu1Qu3sWeBQyno483+DGhHQ90WqaDHsXzubbp+GsHhf7ubD/Og6rH8fprWELV8U+HpCiNtJEm0Hq49GcSo6mae71ir6KvSJf9T7un2LHpgQQog7q9cPkiLh8kH1eXI0/Nof4k7BQ3Ogdg/zzufirVakb6iDNkeFVkPBO4BPArZR1sOZ5+ceYNA3W9lxJu72g7PSYeEE2PSxGra151dZjRaiCCSJtjFd15kecoqqZd3o26hC0U8Y9g94B4J/ESYdCiGEKJg6vUEzqNXoq5fh136QcB4emqfKPWzN6ADBj+EdtZMlQz354sGmxCVnMOKHHTw2M5QzMcnquJRYVW5yeAF0fwsGfqk2pJ8JsX3MQpQSkkTb2NZTcRyITOTJzjVxMBbxnz8zFc5sUD2fZaOgEEJYn3s51e7u0AKVQCdegFHzoUZn+8XUYgw4umHY9QNDmldh/ZQuvHRfXbafjqPXF5uYNncFOT90y+0Y8it0fEENj3H1hb2/2S9uIUo4SaJt7NuQU5T3cub+FpWLfrKzGyE7XeqhhRDClur1g/jTaiX64YVQrYN943Eto7p6HJoPKbG4OBqZ2LUWG6Z04dV6UYw5OoGExAQWN/me9DoD1WMcnKHpSLVJMjnGvvELUUJJEm1D+8KvsO10HI91rIGzg7HoJwxbqeraqtr5F7gQQtxLGj+g9qGMXgxV29o7GqX1k2qiYugv/97kd3Iu489Nwdm3Ch9Wnsbz2xzp8flGlh24iK7ragXblKV6WAshzCZJtA19G3IaHzdHRgYHFv1kJhOcWAW1uoODU9HPJ4QQomA8y8PI2RAQbO9IrvOrCzW7w+4f1QbCNW/B0slQrSPOT6zl08cG8vv41ni6ODJ59j6GfLuN0FR/CGgNe2fe3G1ECFEgkkTbSNjlq6w5GsUj7arh7myBQZEX90FylHTlEEIIobR5Sv1d+KELbJ0KLcepem0XbwA61C7H8skd+GRYEy4lpjHsu+3MzOgEcSchfLtdQxeiJLJIEq1pWm9N08I0TTuladp/8rhf0zTtq9z7D2qa1uKW+42apu3TNG25JeIpjr7beBo3JyOPtKtmmROGrVQ7xGv3tMz5hBBClGw1u0PZ2hBzHHq9D/2/uG08udGg8UBQABumdOH5HnX48nIjknVXDiz9ioTUTDsFLkTJVOQkWtM0I/AN0AdoAIzUNO3Wfmt9gNq5b48D02+5/1ngWFFjKa4i4lNZeuAio1oH4uNmodKLE/9AYFtwK+LIcCGEEKWDwQAj/oTxq6HdpDt2bXJzcuDZHrX5e0pvDpftRZ3YdfT/eAUztpwlM9tkw6CFKLkssRIdDJzSdf2MruuZwBxg0C3HDAJm6soOwEfTtIoAmqZVAfoBP1kglmLp+02nMWoaEzrWsMwJE8Ih6rBqbSeEEEJc41fHrFptfy8X2gx7Hlctkwk+oby7/Cg9v9jI34cuqc2HQoh8WSKJrgxE3PB5ZO5tBT1mKvAyUCqf+kZfTWdeaCRDW1ahvJeLZU4aljulsI4k0UIIIYqoYjOo0JhHXDbx67hWODsYeOqPvQz/fjv7IxLsHZ0QxZYlkui8Xi+69elrnsdomtYfiNZ1fc9dL6Jpj2uaFqppWmhMTMnpaTljy1myc0w82dlCq9Cg6qHL1i70mFghhBDiX5oGLcbC5YN08bzAymc68uH9jTkbm8rgb7YyefY+IuJT7R2lEMWOJZLoSCDghs+rABcLeEx7YKCmaedQZSDdNE37Pa+L6Lr+g67rQbquB/n5+VkgbOszmXT+2neBHvXLU7Wsu2VOmp4E57bIgBUhhBCW0/gBcHCFvTNxMBoYGRxIyEtdmNytFmuOXqb75xv58O9jJKVn2TtSIYoNSyTRu4HamqZV1zTNCRgBLL3lmKXAmNwuHW2ARF3XL+m6/qqu61V0Xa+W+7j1uq4/bIGYioWDFxKJSsqgT+MKljvp6XWqOb60thNCCGEprj7QcDAcnA+ZKQB4ODvwYq+6bJjShf5NKvLDpjN0+SSE37adIyunVFZgCmGWIifRuq5nA5OAVagOG/N0XT+iadqTmqY9mXvYSuAMcAr4EXi6qNctCVYfuYzRoNGtbnnLnTTsHzXitUoxavIvhBCi5GsxBjKvwpG/brq5orcrnw9vxrJJHahb3pO3lh7hvqmbWHM0SjYfinuaVhL/AwQFBemhoaH2DuOueny+kfJezvwxoY1lTpiTDZ/Wgtr3wf3fW+acQgghBKiphdNaqdap41fnc4jOumPRfPD3Mc7EpNCmhi+v921A4yreNg5WCNvRNG2PrutBt94uEwut5HRMMqeik+nVwIKlHBE7Ie2K1EMLIYSwPE1Tq9EROyH6eD6HaPRoUJ5Vz3Xi3UENORGVzIBpW3hh7n4uJqTZOGAh7EuSaCtZczQKgJ4NLFjKceJvMDiqqVRCCCGEpTUdqf7O7J15x8McjQZGt61GyEtdeKpLTZYfukTXT0P4ZNVxkjOybRSsEPYlSbSVrDkaRePK3lTycbXcScP+hmodwMXLcucUQgghrvHwg3p94cBsyM646+FeLo680rse61/sTO9GFfhmw2m6fLKBP3aeJ1s2H4pSTpJoK4i+ms7e8CuWXYWOPQlxp6QrhxBCCOtqMQbS4uH4igI/pEoZN74c0Zy/Jranejl3Xl98mD5fbmZDWLRsPhSlliTRVrDuWDS6Dr0aWrIrx9/qvdRDCyGEsKYa3cA7EPb+ZvZDmwX4MO+Jtnz3cEuyckyM+2U3o2fs4ujFJCsEKoR9SRJtBauPXCbQ14265T0LfxJTDqTEQUwYnN8GRxZB+UbgE2i5QIUQQohbGQzQ/GE4EwJXzpn9cE3T6N2oAquf78yb/Rtw+GIi/b7ezMsLDhCVlG7xcIWwFwd7B1DaJGdks/VUHGPaVkXT8pp2fguTCbZ8Bhf3Q2qcekuJVV04bp2e3uU1a4QshBBC3Kz5KNj4EeydBd3fKNQpnBwMPNqhOkNbVGHahpP8uu0cyw5c4vFONXiicw3cnCQFESWb/ARb2MawGDJzTPRqWMDWdseWwPr3oGxt8KwA/g3Arax6cy+X+7EvuJWD8g2tG7wQQggB4F0FavWAXT9CzPEb/ibl/l1yL6s+vnabo0v+p3Jz5PV+DRjdphr/++c4X647yexd4UzpVZehLatgNBRgwUmIYkiGrVjYc3P2selkLLtf73H3Xww52fBtGzAY4alt6r0QQghRHESGwrp3IDnm+iulek7ex7Z7Bnq9W6DT7jkfz3srjrEvPIF6FTx5vV99Otb2s2DgQlhWfsNWZCXagrJyTKw7Hk3vhhUK9sz6wGyIOwkP/i4JtBBCiOKlShCMXXb9c5MJ0hOulx2mxqqPT66BbV9D42FQseldT9uyqi+LnmrHikOX+N8/xxk9Yxdd6vrxWt/61CnKXiIhbEySaAvaeSaeq+nZBSvlyM6AkI+gUguo19/6wQkhhBBFYTDklhf6Qrna129vMFhtgP/nNXhkuZp8eBeaptG/SSV6NijPzG3n+Wr9SXpP3cSDrQJ5oWcd/Dydrfd1CGEh0p3DglYfvYyro5GOtcvd/eDQnyEpErq/WaBfOEIIIUSx5OoD3V6H81vg+HKzHursYOSxTjXY9FJXxrStxvzQCLp8soFp60+SlplP6YgQxYQk0Rai6zqrj0TRqU45XBzvUpqRkQybPoXqnaBmV9sEKIQQQlhLi0fArz6s/m+BJh3eqoy7E28PbMjq5zvRoXY5Pl19gm6fhbBobyQmU8nbuyXuDZJEW8ihC4lcTkqnV4MClHLsmK5qybq9af3AhBBCCGszOsB976u+0ju/L/Rpavh58P3oIOY+3gY/T2demHeAgd9sYfvpOMvFKoSFSBJtIWuORmE0aHSr53/nA1PjYdtXULcfBLSyTXBCCCGEtdXqDrXvg02fqI4eRdC6Rln+ero9Ux9sRnxyJiN/3MGE33ZzKjrZQsEKUXSSRFvI6iNRtKpWhjLuTnc+cOtUyLgK3f5rk7iEEEIIm+n1HmSlwob3i3wqg0FjcPPKrJ/ShZd712XHmXjum7qJN5ccJi7Z/JIRISxNkmgLOBebQljU1buXciRdgp0/QJPhUL6BbYITQgghbMWvDrSaAHt/g6gjFjmli6ORp7vUIuSlLowMDuCPneF0+SSE6SGnSc+SzYfCfiSJtoA1R6MA6Nmg/J0P3PwpmLKgy39sEJUQQghhB51fAWcvWPUaWHCgWzkPZ94b3JhVz3UkuLov//vnON0/28iS/RcoiYPjRMknSbQFrD56mQYVvQjwdcv/oPizsOdXaDEWfGvYLDYhhBDCptx8ocurcCYETqyy+Olr+Xsy45FW/DGhNd6ujjw7Zz+Dv93G7nPxFr+WEHciSXQRxSZnEHr+Cr0a3mUVOuQjMDhAp5dsE5gQQghhL63GQ9nasPp1yM60yiXa1yrHsskd+GRYEy4npvHAd9t5ctYezsWmWOV6QtxKkugiWn8sGl2/SylH1FE4OBeCHwevirYLTgghhLAHoyPc9wHEnYLdP1nvMgaNB4IC2DClCy/0rMOmkzH0/GIj/7fsKAmp1knehbhGkugiWnc8iso+rjSo6JX/QRveB2dP6PC87QITQggh7Kl2T6jZDTZ+pNq7WpGbkwPPdK9NyJQuDGtZhV+3naXTxxv4afMZMrJl86GwDkmii+hkdDJNA7zR8hvdHRmqxqC2m6zqxIQQQoh7gaap1eiMqxDyoU0u6e/lwof3N2Hlsx1pFliG91Yco+fnm1hx8JJsPhQWJ0l0EZhMOpHxaflvKMxMhZVTwK0ctHnKtsEJIYQQ9uZfH4Iehd0zIPq4zS5br4IXMx8N5rdHg3F1NDLxz70M+247e8Ov2CwGUfpJEl0EUVfTycwxEZhXEm3KgUWPwcX9MOBLVc4hhBBC3Gu6vAZOHrDa9kPGOtfxY+WzHfno/saEx6dy/7fbmPTnXiLiU20eiyh9JIkugvA49Z8wzyR69RuqjOO+D6B+fxtHJoQQQhQT7mWh88twag38+aBaXLIho0FjRHAgIVO68Ey3Wqw9FkX3zzbywcpjJKZl2TQWUbpIEl0E5+PzSaJ3fg87voHWT0Lbp+0QmRBCCFGMtH4Sur8J4Tvgh84w+yG4dNCmIbg7O/BCr7psmNKFgc0q8ePmM3T5ZAO/bj1LVo7JprGI0kGS6CKIiE/FaNCo5ON6/cbjK+Gf/0DdfmoVWgghhLjXGR2g44vw3EHo+jqc2wLfd4S5o1UbWBuq6O3Kpw80ZdmkDtSv6MXby47S64tNrDpyWTYfCrNIEl0E4fGpVPJxwdGY+894YS8sHA8Vm8HQH8FgtGt8QgghRLHi4q1KO547qMaDn94A09vB/HEQE2bTUBpV9uaPCa2ZMTYIgwZPzNrDiB92cDAywaZxiJJLkugiCI9PvV7KceW8qvVyLwcPzQUnd/sGJ4QQQhRXrj7Q9TWVTHd8QY0H/6Y1LJwAsSdtFoamaXSvX55/nuvEu4MbcSo6mYHTtvL83P1cSEizWRyiZJIkuggiriXRaVfgjwcgJwNGLQAPf3uHJoQQQhR/br6qVvq5Q9D+WTi+Ar4JhgWP2rTMw9FoYHSbqmx4qQtPdanJikOX6PZpCB//c5yr6bL5UORNkuhCSsnIJjY5k6o+jqqmK/4MjPgT/OraOzQhhBCiZHEvCz3fgWcPQrtn1Mr09LYwZ5RNu3l4uTjySu96rH+xM30aVeDbkNN0+SSEWTvOky2bD8UtJIkupIgrqjNHz4vfwbnNMPhbqNbBzlEJIYQQJZiHn0qmnzukaqbPblbdPP4YDhG7bRZGlTJuTB3RnCUT21PTz4M3/jpM7y83s/54lGw+FP+SJLqQzuf2iK6YuA+qd4Ymw+0ckRBCCFFKuPmqmunnD0G3NyByN8zoATMHqc4eNtI0wIe5T7Th+9EtyTHpPPprKA/P2MmRi4k2i0EUX5JEF9K1aUcumVfAs6KdoxFCCCFKIRdv6DRFrUz3fFfVSf/aD37uAwnhNglB0zTua1iBVc914q0BDThyMYn+X29hyvwDXE5Mt0kMoniSJLqQwuNT8XJxwJAaqzpyCCGEEMI6nD2g/TOqm0efT+DSfgj5n01DcHIwMK59dTZO6cpjHWuwdP9Fun4awudrTpCSkW3TWETxIEl0IYXHp1Lb1wBZqeBW1t7hCCGEEKWfoyu0fhwaPwBHFkF6ks1D8HZz5LW+9Vn7Qme61ffnq3Un6fJpCHN2hZNjknrpe4kk0YUUHp9Kfa9M9YmsRAshhBC202KsWsQ6NN9uIQSWdeObh1qw8Kl2BJRx5T+LDtHvq81sORlrt5iEbUkSXQgmk05kfBo13TPUDe5+9g1ICCGEuJdUbgHlG8He3+wdCS2rlmHhU+345qEWpGRm8/CMnUz6cy9RSVIvXdpJEl0Il5PSycwxUd1FbS7ETVaihRBCCJvRNLUafemATftI5x+ORr8mFVnzfGee61Gb1Uej6P7ZRn7ecta6/aV1HfbOgsRI611D5EuS6EIIz+3MUckpRd3gLjXRQgghhE01eQAcXGDvTHtH8i8XRyPP9ajD6uc60aJqGf5v+VEGfbOVfeFXrHPBU+tg6STY/Ll1zi/uSJLoQriWRPsZktUNshIthBBC2JZrGWgwWNVFZ6bYO5qbVCvnzm/jWvHNQy2ITc7g/unbeG3xIRJTLThC3GSCtW+rj8P+VqvSwqYkiS6EiPhUjAYNL1MCGJ3B2dPeIQkhhBD3nhZjICMJjvxl70huc63EY+0LnXm0fXXm7o6g22chLNwTaZmph0cWQdQhqNUTrl6Ei/uKfk5hFkmiCyE8PpVKPi4Y0+JUZw5Ns3dIQgghxL2najsoW7tYbDDMj6eLI2/0b8CySR2oWtaNF+cfYMQPOzgfV4TV8+xMWP8ulG8Mg6eDZoCwlZYLWhSIJNGFEB6fSqCvG6TESo9oIYQQwl40Ta1GR+yE6GP2juaOGlTyYsGT7fjo/sYcu5REv6+2sOzAxcKdbO9vcOUc9HgLPPwgoA0clyTa1iSJLoSI+FQCfd0hNVba2wkhhBD21OwhMDgWqw2G+TEYNEYEB7Ly2Y7UKe/B5Nn7eHXRIdKzcgp+kswU2PgxVO0AtXqo2+r1hegjKrEWNiNJtJmSM7KJTc68vhItg1aEEEII+3EvB/X6wYHZkJ1h72gKpEoZN+Y+0ZYnO9dk9q5wBn+zlVPRVwv24B3fQkq0WoW+Vk5at696H/a3dQIWeZIk2kwRuZ05rpdzSBIthBBC2FWLMZB2BY4ts3ckBeZoNPCfPvX4dVwroq9mMODrrSzYc5d+z6nxsPUrqNsPAoKv3162JvjVg+MrrBu0uIkk0Wa61t6uqpcBslKkR7QQQghhbzW6gk9gsd5gmJ8udf35+9mONKnizZT5B3hh3n5SMrLzPnjL55CZDN3fuP2+un3h/DaVaAubkCTaTP+uRF+bVig10UIIIYR9GQzQfAyc3QTxZ+wdjdnKe7nw52NteKZ7bRbvu8DAaVs4dinp5oMSI2HnD9B0JPjXv/0kdfuCngMn19gmaCFJtLnC41PxcnFQPaJByjmEEEKI4qD5KNXqrQRsMMyL0aDxQs86/DG+NUnp2Qz+Zit/7gy/3lM65CNAhy7/yfsElVuCR3kIk5IOW5Ek2kzh8alULeuu6qFBNhYKIYQQxYFXJah9H+z/E3IsOBnQxtrVKsfKZzoSXN2X1xYf4pWFB8m4dBT2/wGtJqiylbwYDFCntxoFXkI2WJZ0kkSbKTzuhh7RIH2ihRBCiOKi5VhIjoITq+wdSZH4eTrz67hgJnWtxbzQSPb88iImRzfo+OKdH1ivn6qZPrvZNoHe4ySJNkOOSSfyShoBvm6qRzRITbQQQghRXNTqCZ4VS+QGw1sZDRpT7qvL7L6OtMvcxg/Z/dgZdZcJydU7g6O7lHTYiEWSaE3TemuaFqZp2ilN024r1tGUr3LvP6hpWovc2wM0TdugadoxTdOOaJr2rCXisZaopHQyc0y5K9ExYHQCZ097hyWEEEIIAKMDNBsFp9aqjXglna7T9sxXZLuWZZnbYEb9tJNftp69Xid9K0cXqNVN9Ys2mWwb6z2oyEm0pmlG4BugD9AAGKlpWoNbDusD1M59exyYnnt7NvCiruv1gTbAxDweW2yE39QjOk5tKtTu8qxQCCGEELbTYjToJtj3+92PTUuAI4sh7rTVwyqU0+vh3GYcurzC7Ek96VLXn3eWHeXFeQfyn3JYtx9cvQSX9tk21nuQJVaig4FTuq6f0XU9E5gDDLrlmEHATF3ZAfhomlZR1/VLuq7vBdB1/SpwDKhsgZis4qYkOjVWekQLIYQQxU2Zaqpv9N5ZYMoj0UxLUJsP/3gAPqkF8x+BH7rA6Q22jfNuTCZY947aSNjyEbxcHPlhdEue71GHRfsuMHT6tn/b7t6kzn2gGeH4StvHfI+xRBJdGYi44fNIbk+E73qMpmnVgObAzrwuomna45qmhWqaFhoTE1PUmAslIj4Vo0Gjko9L7shvqYcWQgghip2WYyEp8npi/G/iPFwlzn89BdHHoc2T8PAi8A6AP4bBvj/sGvZN9s2ESweg6+vg4AyAwaDxbI/azBgbRHhcKgOnbWHrqdibH+fmC4FtIUySaGuzRBKdVz3DrcU6dzxG0zQPYCHwnK7rSXkci67rP+i6HqTrepCfn32S1/D4VCr7uOJgNKiaaOkRLYQQQhQ/dfuq7lkhH9ySOB9TifOE9fDcQej1HtTqDo/+DdU6wJKnIeR/kF/NsS0kx8CC8bDsWagSDI0fuO2Q7vXLs2RSe8p5ODN6xk5+3HTm5jrpen0h+ijEn7Vh4PceSyTRkUDADZ9XAS4W9BhN0xxRCfQfuq4vskA8VnP+Wns7gNQ46REthBBCFEcOztB8NFzYk3fiXKXlzXuaXLzhofnQ9CGVeC+ZZPte07oO+2fDN63g6BLo8io8shwMxjwPr+HnweKJ7bmvYQXeX3mMZ+bsJzUzd1x43T7qfdjfNgr+3uRggXPsBmprmlYduACMAB665ZilwCRN0+YArYFEXdcvaZqmATOAY7quf26BWKwqIj6VXg0rQFa66sMoPaKFEEKI4qnra2pEtl/dgjUBcHCCwd+qGuSNH0HSBRg+E1y8rB/rlXOw/Hm1kbBKMAz8Gvzr3fVhHs4OfDuqBdM3nuaTVWGcjLrK96NbUrVsDfCrr0o62j5t/fjvUUVeidZ1PRuYBKxCbQycp+v6EU3TntQ07cncw1YCZ4BTwI/Ate9oe2A00E3TtP25b32LGpM1JGdkE5eSeX1TIUhNtBBCCFFcOTirRNScLlqaBl1fhUHfwLnN8EsfSLxgvRhNObD9G/i2LUTsgr6fwqOrCpRAXw9Z4+kutfh1XDCXEtMZ8PUWQsKiVUnH+W2QGm+9+O9xlliJRtf1lahE+cbbvrvhYx2YmMfjtpB3vXSxE3FTe7vL6kYp5xBCCCFKn+YPq6Et88bCTz1g1Hyo0Miy17h8GJZOhot71bjufp+Bd5VCn65zHT+WTerA47NCGffrbj5u04QH9Bw4uRqajrBg4OIamVhYQNfa2wX4uqoe0SAbC4UQQojS6tqGQ1Ar0pZqgZeeCGvfgR86Q0I4DPsZRs4pUgJ9TWBZNxY93Y7+TSrx8nYjCcayZB9dboGgRV4kiS6gCl4uABy/dPWGcg5JooUQQohSq0JjmLBWtcD7/X6YPw4i9xTuXKnxsP59+KIxbPkcmjwIk3ZDo6EWHdzm5uTAVyOa8Xq/hqzMbEZW2BrOXoq9+wOF2SSJLqAmVbypXs6dxfsuqB7RIEm0EEIIUdp5V1Yr0m0nwal18FM3mHEfHF2a9zCXW12NgtX/hS8awaaPoUYneDxEbWJ087VKyJqmMaFjDZr1eAhX0vn4u59YdyzKKte6l0kSXUCapjG4WWV2nI3javwlMDiCsw127AohhBDCvly8ode78MIR6P0/NVZ73mj4qjnsmA4ZV29/TEIErJgCUxurzYP1+sHTO+DB36FSc5uE3aD9AEyO7gx02cf430KZuvYEJpMde2CXMpJEm2FQs0roOkReiFCr0BZ8+UUIIYQQxZyzp+o5/cw+GD5LbT785z/weQO12pwQAXGnYclE+KoZ7PkVmj4Ik0Jh6I/gX9+28To4Y6jdg96O+xnavCJT157k8VmhJKXbuAd2KWWR7hz3imrl3Gke6ENizCUoK6UcQgghxD3JYIQGA9VbZKhaad7+rXpDB6MTBI2H9s9YZMNgkdTti3Z0CZ+2N9EkoCHvLj/K4Glb+X50S2qX97RvbCWcJNFmGtK8Ms5/x5PsWBEPewcjhBBCCPuqEgQP/KJWoUN/Vgl28OPg4W/vyJTavUAzooWtZGz3N6lXwZOJf+5l8Ddb+Wx4U3o3qmjvCEssKecwU7/GFSmrJXE+zdXeoQghhBCiuPAJgB5vQbf/Fp8EGtTmxart4PgKMOXQukZZlk3uQK3ynjz5+14+WXWcHKmTLhRJos1U1sMZP0MyhxMcpThfCCGEEMVfk+EQcxxmDYarl6no7cq8J9owolUA32w4zaO/7iYxVeqkzSVJtLmy0nHVUwlPd2fnWRmlKYQQQohirvloGDgNInbD9PZwcg3ODkY+GtqED4Y0ZtvpWAZM28KxS0n2jrREkSTaXLmDVpKNPvy174KdgxFCCCGEuAtNgxaj4YmN4FEe/himuolkZ/JQ60DmPN6W9Kwc7v92G8sOXLR3tCWGJNHmyh20UjUwkJWHL5GeVYBG60IIIYQQ9uZXFx5bpzqHbPsafukN8WdpWbUMyyd3oEElLybP3scHK4+RnWOyd7TFniTR5spdiW5ZvzZX07PZcDzazgEJIYQQQhSQoyv0/xyGz4S4U/B9Jzi8EH8vF2Y/1obRbaryw6YzjP1lF/EpmfaOtliTJNpcKXEANKpTEz9PZzUGXAghhBCiJGkwCJ7YDH71YMGjsPQZnEzpvDu4ER8Pa8Luc1cY8PUWDl9ItHekxZYk0eZKiQHA6OHHoKaV2BAWTUKqPFMTQgghRAlTpiqMWwkdXoC9M+HHrhB7kuFBAcx/oi0mXWfo9G0s2htp70iLJUmizZUaCwZHcPFmcPPKZOXorDx02d5RCSGEEEKYz+io+luPXgzJ0bD4SdB1mgb4sGxyB5oF+PDCvAO8vfQIWVInfRNJos2VEgtuZUHTaFjJi1r+HtKlQwghhBAlW82u0P1NuBAKp9YBUM7Dmd8ntObR9tX5dds5Rv20k5irGXYOtPiQJNpcqXHgXg4ATdMY0rwyu87FE3kl1c6BCSGEEEIUQbNR4B0IIR+ArgbKORoNvDmgAV882JQDEQkM+HoL+yMS7BtnMSFJtLlSYv5NogEGNq0EwJL90ldRCCGEECWYgxN0mgIX9sDJNTfdNaR5FRY+1Q6jQWP4d9uZuzvcTkEWH5JEmyslFtyuJ9EBvm60qlaGxfsuoOsyBlwIIYQQJVizh8Dn5tXoaxpV9mbZ5A4EV/fllYWHeH3xITKz7906aUmizXVDOcc1g5tX5lR0MkcuyrhMIYQQQpRgRkfo9BJc3AcnV992t6+7E7+Oa8UTnWvwx85wRv64g+ikdDsEan+SRJsjOwMykm5Lovs1roijUZMNhkIIIYQo+ZqOBJ+qEPLhbavRAA5GA6/2qc+0h5pz9GIS/b/ewp7z8XYI1L4kiTZH7sjvG8s5AHzcnOhS15+lBy6SY5KSDiGEEEKUYDeuRp/4J9/D+jepxOKJ7XB1MjLihx38vuP8PVXaKkm0OXJHft+6Eg0wpHlloq9mMD3k1D31AySEEEKIUqjpCChTLd/V6GvqVfBi6cQOtK9Vjv/+dZhXFh4kPSvHdnHakSTR5shnJRrgvoYVGNC0Ep+uPsH/LT+KSVakhRBCCFFSGR2h08tw6QCE/X3HQ73dHJkxthWTutZiXmgkD36/nUuJaTYK1H4kiTbHtSTa3e+2u4wGjS8fbMaj7avzy9ZzPDNnHxnZ98YzMSGEEEKUQk0ehDLV77oaDSoPmnJfXb57uCWnopMZ8PUWdp6Js1Gg9iFJtDn+Lecom+fdBoPGG/3r82qfeiw/eIlxv+zmanqWDQMUQgghhLAQowN0fhkuH4TjKwr0kN6NKrBkUnu8XB0Z9dNOftl6ttSWuUoSbY6UWDA4gItPvodomsYTnWvy+fCm7Dobz4Pf7yD66r3Z+kUIIYQQJVzj4eBbAzZ+dNfV6Gtq+Xvy18T2dKnrzzvLjvLivAOlsk5akmhzpMaCW1nQtLseen+LKvw0NohzcSkMnb6Ns7EpNghQCCGEEMKCjA6qNvryITi+vMAP83Jx5IfRLXm+Rx0W7bvA0OnbiIhPtWKgtidJtDlSYvOsh85Pl7r+zH6sDSkZOQydvo0DMmteCCGEECVN4wfAtyaEfASmgk8oNBg0nu1RmxljgwiPS2XgtC1sPRVrxUBtS5Joc6TkrkSboWmADwufaoe7s+qhGBIWbaXghBBCCCGswOgAnV+BqMNwfJnZD+9evzxLJrWnnIczo2fs5MdNZ0pFnbQk0eZIjc2zR/TdVC/nzsKn2lG9nDsTfgtl1vZzMpRFCCGEECVHo6FQthaE/M+s1ehravh5sHhie+5rWIH3Vx7jmTn7Sc3MtkKgtiNJtDlS4vLsEV0Q/p4uzH2iDW1rluWNJUcY8PUWtp0uPS9pCCGEEKIUu7YaHX0Eji0t1Ck8nB34dlQLXu5dl+UHL3L/t9s4H1dy94xJEl1Q2RmQkWhWTfStPF0cmfloMF+NbE5iWhYP/biTx2aGyqZDIYQQQhR/jYZCuTqw4X04OA9OrILwnRB9HJIuQWbqXTt4aJrG011q8eu4YC4lpjPg6y0lttRVK4k1KUFBQXpoaKhtL5p0ET6vD/2/gKBHi3y69KwcZmw5y7cbTpGZY2JM22o806023m6OFghWCCGEEMIKji2HeWNAz6dlncERXLzVW6sJ0PbpfE8VHpfKkzN3ciw6lSm96vJ0l5poBeiAZmuapu3RdT3o1tsd7BFMiXSHkd+F4eJoZGLXWjwQVIXPV5/g561nWbg3kue612ZUm6o4GuVFAiGEEEIUM/X7w0unIDUe0hMhPSH3LfH6W1oCXNwHa95Ux/sE5nmqwMRdrEgZySd1pvLJqjAORSby6fCmeDiXjPS0ZERZHPw7rdAySfQ1/p4ufDS0CWPaVuP9lUd5e9lRZu04z+v96tO1rn+xfEYmhBBCiHuYm696u5PESPiqOWz6BAZ+nfcxB+ehZafxUvYP+Pb9gQ/+DmPwN8n8MLolNfw8LB+3hclyZ0GlJ6r3BuuUWzSo5MXv41vz05ggdB0e/TWUPl9uZtb2czI6XAghhBAli3cVaDkO9v0B8Wduvz8nG8L+Bq/KaBf2MMF9K7+Pb018SiaDpm1l3bEo28dsJkmiC6pqB3BwhdCfrXYJTdPo0aA8q57vxEf3N8Zo0HhjyRFaf7CO/yw8yKHIRKtdWwghhBBWlhwNvw/LO6ksjTq+AEZH2PjJ7fdF7IC0eLjvfQhsB2vfpl0lA0sntadqOTfG/xbK1LUnMBXjlsCSRBeUhx+0fAQOzoUr5616KUejgRHBgSyf3IElE9szoEkl/tp/gQHTtjBw2hbm7g4v8b0VhRBCiHvO/j/h1BrY+qW9I7ENzwpqc+HBORB76ub7jq8AozPU6gF9P1Gv+K9/jypl3FjwZDvub1GZqWtP8visUJKK6SvykkSbo91kMBhh61SbXE7TNJoG+PC/YU3Y+VoP3hnYkPSsHF5ZeIjW76/jzSWHOXoxqVRM/RFCCCFKvUML1PsDc9TGvHtB+2fBwQU2fnT9Nl2H48uhRhdw9oQKjSD4MfVq/8V9uDga+eyBprwzsCEhYTEMnraVk1FX7fYl5EeSaHN4V4Zmo2Df76rlnS0v7erI2HbVWPVcJxY82ZYeDcozZ3cEfb/aTOdPQnhv+VF2nomTSYhCCCFEcRR9HKIOqVe1s9Nhzy/2jsg2PPxVgnxogfo3AIg6AgnhUK/f9eO6vKpmcayYAiYTmqYxtl01/pjQmqT0LB6esZP0rHza6tmJJNHm6vAcmHJgWz47Ta1M0zSCqvnyxYPN2Plqd94f0ogafu7M3H6eB3/YQav31zJl/gFWHblMWmbx+mETQggh7lmHF4BmgC6vQY2usOtHyCmeZQoW1+5ZcHK/vhp9fAWgQd0+149x9YGe/wcXQmH/H//e3LpGWZZN7sDnw5vh4mi0adh3I8NWCmPxU3BkMTx3SNVKFwPJGdlsDIthzdHLrDsezdX0bFwcDXSo5UevBuXpUtcPfy8Xe4cphBBC3Ht0Hb5qBmWqw5i/4MRq+PMBGDoDGg+zd3S2se5d2PwpPLkV/noKHF1h/Oqbj9F1+Lk3xJ2EyXvAtYx9Yr1FfsNWJIkujNiTMK2VWpXu8bb94shHVo6JXWfjWXM0itVHLnMxMR2AGuXcaV3Dl9bVyxJc3ZdKPq52jlQIIYS4B0SGwk/dYdC30HwUmEzwTStw9oLH1sO9MBMi7QpMbQK+1eHSAbXq3P7Z24+7fAi+7wRB46Hfp7aPMw8ysdCSytWGhkNg10/qB6CYPFO6xtFooH2tcrSvVY63BjTgyMUktp2OZeeZeJYfvMTsXREABPi60rp6WVpX96VNjbJUKeMqw12EEEIISzs0X3WiqN9ffW4wQOsnYeUUiNwNAcH2jc8WXMtA24kQ8qH6vF7/vI+r0BhaPQa7f4QWo6FiU9vFaCZZiS6sy4fhu/aqtqnLK/aNxQw5Jp1jl5LYeTaenWfi2HUunoRUVZNVyduFpgE+NKrsrd4qeVHWw9nOEQshhBAlWE42fF4fAtvAg7Ou356RDF80gJrd4IFf7RaeTaUnwke5I8DfvsPsi7QEmBakyl8eXaWedNiRrERbWoVGULcv7PgW2j6tWrSUAEaD9m+SPL5DdUwmnRPRV9l5Jp5d5+I5fCGRvw9f/vf4St4uNKzsTaNK3jSu4kWjSt5SWy2EEEIU1LlNkBINjR+4+XZnD2gxBrZ/q0Zke1exT3y2ZLqh4UHkHqjSMu/jXH2gxzuw5Gk4MFuVwBRDkkQXRccpENYNds9Q9dElkMGgUa+CF/UqeDG2XTUAEtOyOHIxkSMXkjh8MZFDFxJZeyyKay9alPNwppa/OzX9PNSbvwc1/dyp5O2KwSDlIEIIIcS/Di1Qtc+1e91+X/DjsP0b1amj5zu2j83WTt6wkXDD+zB6Uf7HNh0Je36FNW+qVniuPtaOzmySRBdFlZbqZZjt09R/BCc3e0dkEd6ujrSrWY52Ncv9e1tyRjbHLiVx+EIiRy8mcTommWUHLpKUfn1yoqujkRp+15PrauXcqFLGjQBfV/w8nKXeWgghxL0lKw2OLoUGA8Exj1dxfQKh/gCVLHZ+WbWBK82OrwDPitD6CVj7NoTvUGUueTEY1MbCH7rAhg+g78e2jLRAJIkuqk4vwS99YO9MaPOkvaOxGg9nB1pV86VVNd9/b9N1nbiUTE5HJ3M6JoXTMcmcjklmX8QVlh28yI3l9s4OBiqXcVVJde77KmVcqVLGlYrerpTzcMLBKG3LhRBClCInV0Pm1Tu3sWvzNBxdoqYYthpvu9hsLSsNTq2Dpg9eX4Hf8AGMXZr/Yyo2VV06dv8IzR+Gik1sF28BSBJdVFXbQdX2sPVLCBoHDvfORjxN0yjn4Uw5D2da1yh7031pmTlEXkkl8koaEbnvI6+kEhGfxqHIBK6k3txg3qCpMpEK3i74e7pQ3suZCl4ulPdyoby3C/6e6jpl3Bwl2RZCCFEyHJoP7v5QrVP+xwS0hkrNYed30HKc3TfRWc2ZjZCVokoznNyhwwuw6lU4twWqdcj/cd1eV7M5Vr4Ej/5TrNoBShJtCZ2mwKwhsP9PlUgLXJ2M1C7vSe3yeW+4TM7IVkl2fBqXk9KJTkrnclI6UUkZRF5JZc/5+NsSbVD/d8q4OeHr7kRZdyfKeThT1iP3cw9nyro74ePqiJerIz5ujvi4OeHuZJRSEiGEELaVlqCGqgSNA+Md0i1NU6vRix6D0+uhdg+bhWhTx5er2vBrTyiCxqkFyA0fwCMr8k+OXcvAfe9D/BkwZYPR0XYx34VFkmhN03oDXwJG4Cdd1z+65X4t9/6+QCrwiK7rewvy2BKhRleo3BK2fAHNR9/5P4sAVHnItQ2N+UnPyiHmagZRucl1fEoGscmZxKVkEJecSVxyJscvJxGXkvlvm768OBg0fNwc8XZVSbW3qyOeLg54OKs3d+frH3u4XPvciIezI+7ORjxz38sKuBBCWElWGpzfBrW62zsSyzm+HHIybu/KkZcGg2H1G6rjV2lMok05cOIfqN0THJzUbY6u0PFF+PslOLNB7THLT9MRtonTTEXO9jRNMwLfAD2BSGC3pmlLdV0/esNhfYDauW+tgelA6wI+tvjTNFUbPXsEHF5QbL/ZJY2Lo5EAXzcCfO++YTMrx8SVlEziUjJJTMsiITWLxLTM3PdZJKRlkZiaRUJaJpcT0zkVnU1KRjZXM7LJzDYVKB5nBwOeLteTbndnBzydHXB1MuLiaMTZwYCzgxFnR8P1jx0MuZ9fu9+As+MNH99yvJODAUejhoPBgINBk24nQoh7w7ZpsOE9mBSqBpqVBocWQJlqapHtbhycIHgCrH8Poo+Dfz2rh2dTkbshJUaVctyo5VjY9pX6umt0LValGgVhiSXTYOCUrutnADRNmwMMAm5MhAcBM3U12WWHpmk+mqZVBKoV4LElQ+37wL+hWo1uPLz01jQVU45GA/5eLoXqYZ2VY1IJdXo2KZnZ1z/OyPk30U7JyL794/Rsoq6mk5qRQ0a2KfdNfVzQxPxuNA0cDQaMBg0Hg4aDUcOYm2A7GNVt6j7DzZ8bDf9+rGkaBg0Mue9v/lxDu+U27YZj1efXHgca5v2CM/f3obm/PqVMp2BK4lAtS7l3v/KSZeKxPykP/LVkPnvLDbR3OEXmkRXHlDMb2VR+NOuXHinQY9yy2vCi5sT+uR+wLPBlK0doW70u/EobzYH/nahCxunDN93X3PthhoR/yJ8zv+O4T8d8z1HGzYnne9axdqhmsUQSXRmIuOHzSNRq892OqVzAxwKgadrjwOMAgYGBRYvYGgwG6PgCLBwPYSuvj/YUxZ6j0YCPmxM+bk4WO6fJpJOZk5tYZ+X8m2CnZ92cbGdk3fBx7rHZJp0ck052jk62yfTv51k5JnW7SSc7x3TbceqY3NtMKpHP0XV0XSVRJh1Mue/V59dv0/N5f/Px5v0bmJu4mZvsmJsX6rp+Tyfd9/CXbvaTs2JH1y3yDXTXU3lJ/4UlWjf2afUtEJhl1NAjKK/W0tAidrLswm2D4UqckfpKDJj4KroZZ2MuFvhxdfUO9I77m5fiB5GolYwhbnel60zQ17OThiw4nAjcPKlwpd6U5lSk1dlv+FSrjq7lvQhZpYxbqUyi8/qffeuft/yOKchj1Y26/gPwA6ix3+YEaDMNBquXJDZ/pl6yuJf/at3jDAYNF4Mq88C1+GyCEEKUMLt+hK1fwZObiz5sYt27sDmEQYZtMOQ7aDTUEhEWXchHEKJB5ZYMSotg0DN5DCUpaX78H+Q0ZtGTj5r3uKjKML0dId3Oq4W50iD6GHwbRWC/V9jXKp/v7aEUWDievUOuQpMC1JAXE5aoOYgEAm74vApw69Ou/I4pyGNLDqODmlx4cS+cCbF3NEIIIUq6wwshMVwtzhTF1ctq01rdvlA5CBY8qpLz4lDqc2SxahfbYCDEn4bkGHtHVDTxZ+BCaME2FN6qfEOo3lk9ecrJf8N8iXJ8hXpft2/+xzS8H8o3gpAPStTXbYkkejdQW9O06pqmOQEjgFs7Zy8FxmhKGyBR1/VLBXxsydJ0pJrGU9RfeEIIIe5t6YkQsQsc3VQP4SvnC3+ujR9DTib0eg9GL4aGQ2DNG/D3y6pzgr1EH4OY4yqegNzJdRE77BePJRxaqN4XdqW/zdNw9aIawFIaHF+hNld6Vcz/GIMBuv1XPQHZ/4ftYiuiIifRuq5nA5OAVcAxYJ6u60c0TXtS07RrI/xWAmeAU8CPwNN3emxRY7IrB2doNxnObVa//IQQQojCOLsJ9BwY8BVoRlj3TuHOE3ca9v4GLR+BsjXV+OmhP0PbSbDrB5g7GjJTLRp6gR35C9Cg/kCo1AyMzmoUdEml63BonhrC5l2lcOeo3Qt8a6iJfibLbFK3GF2HeWNgenvVkvBuEi+oV+dv7cqRlzq9oUor9YQvK73osdqARVpI6Lq+Utf1Orqu19R1/f3c277Tdf273I91Xdcn5t7fWNf10Ds9tsRrMRZcfWHz5/aORAghREl1ah04eULDwdBukirtiAy968Nus/49MDpBpxs6PhgMaoBFn4/VZvjfBkBKrMVCLxBdV6Uc1TqAZ3m1CFWpOUTstG0clnT5EMSeuPOY77sxGNQ0v4t7Vfu34mTfLLVCnhgBv/SBpZMhNT7/48NWqvf1CtBsQdOg2xuQdAH2/GKZeK1M+rBZg7MHtHkKTvwNUSV7YV0IIYQd6DqcXgfVO6kJbe2fVeOjV71uXh3zxf1wZBG0nagS1Vu1fgIenAVRh2FGT7VqbSvRxyA2TD1JuCawtYo5K812cVjSoflgcFCNBoqi+cOqxGXd/6mx2EWRkw2hv0BydNHOkxAO/7wG1TrC80eh3TOw7w+Y1goOzM375zJsJZStBeUK2FWjRmdVE77pU8hILlq8NiBJtLUEPwZOHqpvtBBCCGGOuNMqaanZVX3u7AldX1P1wseWFfw8695Rr4y2eyb/Y+oPgLHL1JjqGT0hYneRQi+wI4tBM6hSjmsC2oApCy7us00MlmQyqVcLavUAN9+inUvTYODXqqxjwaNwNarwMS2dDMufgz8fLPyTE5MJlkwEdBj0jVos7PUuPLFRDZRZ/DjMHHTzk7C0BFWSVLeved3Kur8JqbFqH0AxJ0m0tbiWgaBH1X+o+DP2jkYIIURJcnqden/jGOzmo8GvHqx9C7Iz736OMxvh9HroNAVcvO58bEAwTFgLzl6qtOPY8sLHXhDXSjmqtgcP/xviyB0VURLrosO3q1KEwnTlyIuzJwyfCelJagZFTrZ5j9d1tXn0wJ/qicrFfbD0mcJ1ZNn9k0qI7/sAylS9fnuFxjB+NfT7TJ3/27aqpjk7A06tBVN2wUo5blQlCOr0Ud1j0q6YH6sNSRJtTW0ngsERtn5p70iEEEKUJKfXQ5nqaiXyGqOD6q4RfwZCZ9z58boOa98G7wAIGl+wa5atCePXqDZr88fClXOFjf7uoo5A3ElVsnAj97LqpX9b1EWnxKl6cUuVjhyarzqp1O1jmfMBlG8AA6aqZgUbzNw2tnUqbJ8GwY+rZLz7G2rTo7k5Sdxp9cStVk9oMeb2+w1GaDUBJu2Gen1VnN91UCvJ7v4qKTZXt/9CRiJs+9r8x9qQJNHW5FlB1TXt/xOSSm77ayGEEDaUnQlnN9+8Cn1NrR5Qowts/N+dV+mOLlEb07q+prpxFJSHn6qR1gzWTWCO/nV7Kcc1Aa3VSrS1O1Ps+AY2fQJ7fiv6uTKSVe153b7g5F70892o6QjVWWXL5xD2T8Ees3emehLVaBj0/p8qp+jwgmq7t/ZtOLGqYOcx5cBfT6u6/IFf3bksw7MCPPArjFoA2ekQuVs9oTAYC3atG1VopGLdMb3otdxWJEm0tbV/Rv0Qbv/G3pEIIYSwBZOpaAlgxA7ISoGaeSTRmqZWo9MS1OarvORkw/p3wb8BNHnQ/Ot7VVIzD/bOKnwt7p3825Wjo0rabxXYBtITVJcLa8nOVF8fqNXaog742POr6uvd+sm7Hloovf8HFZqo2uO7vUJwbBkse1b9/Ayerrp9QG6d9TSo2AQWjIeYsLtfd/s36uexzyfq56IgaveEp3dC30+h88t3Pz4/XV5TZSHFuNOZJNHWVqaaqo8K/fnObWCEEEKUfEeXwGd14J//FP4cp9apDg/VO+Z9f4XG0HyU6vEcf/b2+/fNgrhTaoNWYVYBQXUDMWWpKYeWFnVYxXdrKcc1thi6ErYCUqJVqUtiBBxeVPhzZWeoRLxaRwhoZbkYb+TookoydGDeWHXNvJzdrBLkyi3VKwoOTjff7+QGI/4ER1eYPeLOeUn0cVXuUq8/NBluXrxObqrBQmF7ZQOUqwXNHlKlSwkRhT+PFUkSbQsdnoes1BKx01QIIUQhpMTB/HFqEEXGVfVyelpC4c51ep1KJJ098z+m639Vor327Ztvz0yFkI/U4+v0Ltz1QdVHNxwCu2dYfnPXkcVqeEz9Aflf260chFuxLjr0F/AOVH2y/eqrOuHCjkA/MAeuXlJ/663JtzoM+Q4u7Yd/Xr39/ov7YfZIddxD8/IvK/GuAiP+gMRIWDAu7w2LOdnw15PqHP2/MK+7hiV1fkW93/Sxfa5/F5JE24J/PfVMbud36perEEKI0uPoUvi2tXoZvet/4ZGVkJ0GB+eaf67kaDWwo1a3Ox/nVVG1rTv6183TcXd+B8mXoec7RU98OrwAmVdh109FO8+NrpVyVO8E7uXyPkbTVF20tVai407D2Y3QcozarNn+GYg+orpJmMuUoxLwik2h5l2+Z5ZQr696lSB0Bhycd/322FPw+1Bw9YGHF929xV5AsEqOz4TA6v/efv+WL1S3jf5f3Nw9xdZ8AlSns31/2LaHeQFJEm0rHV9Q9VKhJWMKjxBCiLtIjVc9fOeNBs+K8HgIdH4JqrSESi1UGZ+5q5un16v3BUnI2k0Gj/Kw6jV1ndR42DJVtQcLbGPuV3O7Co2g9n2wczpkphT9fACXD6ruIjcOWMlLYGt1nDU2le35Ra3iNx+tPm80DLwqF66T1tElEH9aPeGw1WpttzdVa8Blz6qSi6SLMGsIoMPoxeBduWDnaf4wtJmovr97Z16//dJBtXG10dC7f59soeOLaprlhg/sHcltJIm2lcot1Y7q7dNKzEx4IYQQ+Ti2DL4JVqvQXV+Hx9arpPOaoHEQc9z8fsen1qlShgpN736ss4dqBRa5W61Ib/kCMpJUKzNL6fgipMbdnGQVxbVSjnr5lHJcE9hWvbd0q7usdLWqWa+f6iYBqm64zdOqjVzknoKfS9dVx4yytfIvTbEGowMM+1kNdJs3GmbdD2nxqitGudrmnavn/0GNrrD8BfWzmp0Jfz2lVrL75rNx1dY8/NWGzZSYom8AtTBJom2p44uQHAX7f7d3JEIIIQojNR4WToC5D9+w+vyyagF2o0ZD1eCSPWa8+mgyqZXoml2vd1S4m2ajwL8hrPqv2mjYdITq82wpga3Vque2rws24OVOdB2O/KVGO7uXvfOxFZuC0dnyQ1eOLVUJZ8txN9/eciy4eMNWM6YMn1qnSm/aP1f4DZyF5VlBJdJxp9RK+Ig/oHIL889jdIAHflFlE3MfhpUvqo2fA74s+tRFS+r6Ooxdevv/MzuTJNqWqnVUmz1WvwGnN9g7GiGEEOY4txW+aa1WU7u8dvvq842c3FVHgyN/FbwzU9QhNe44r9Z2+TEY1fjlpEjQTaovtKV1fEFN4itMjfeNLh2AK2fz78pxIwdnlRRaOokO/UUNsKne+ebbnT2h1WNqUmPsqYKda8vnqgykMG0ELaF6R3jwD1XCUaNL4c/jWgZGzlEdP/bOVE/MLDkwxhKMDvaOIE+SRNuSpqmWM2Wqw5/D4fgKe0ckhBClS8ZV1eZr65eqU8ZXLSDkf0U/b/xZmPOQWq18PAS6vHL3VbGW4yAnQw3cKohTuaO+zd2gVqu7Kkfo9T74BJr32IKo2V31KN46VW2kK6wji1UtckHHQAe0Vom3pSYKRh+D8G1qcEleK/2tnwCjE2z76u7nCt8J57dC20m3t5GzpXp9oVqHop/Hr65qoddgEPT+sOjnu0dIEm1rHv7wyHLV53PuaDi0wN4RCSFEyZSdqToI7P4J/poI37SBDwPgt/6w5k2VgLn6QMgHRdvUnZmqfl8DjJqnfn8XRIVGUCVYDeIoyAbD0+uhfGPwLG9+jL0/hNaPm/+4gtA0VY4Yd0qVQxTGta4cNboUvEwgsI3qVX1hb+GueavQX1SS3GxU3vd7+Kv+2wdm333IzJbPwdVXlYGUFjW7qkTaxdvekZQYkkTbg5svjFkCVdup2ro9v9o7IiGEKFn2zoIPq8APXWDFi3DiH7UK2+VVtcHqpTPw7AF4dLUalb1yimrnZS5dh2XPqDrRoTNUKYA5gsZB3Ek4t+XOx2Ukq9KFu7W2s5f6A9QGus2fFa6f8sV9kHAeGgwu+GMCWqv3lmh1l5mq+jk3GJR/az1QK8umbNWxIj9RR9TPW+snLT/iW5QokkTbi7MnjJqvfrkvexa2TbN3REIIUTLoOmz6BMrVgQd+hecOwUun1Cpxl1fU2OFrG9eudTIoW1uVd8SeNO9aO76FQ/NVx4vaPcyPteEQtbJ3tw2G5zarVVdz6qFtyWBUw0QuH7pedmKOf0s5+hX8MW6+6ntsiaErRxZBRuLtGwpvVbYm1B8Iu3+G9KS8j9nyheqMEfxY0eMSJZok0fbk6KrGbzYYBKtfhw0fFn5ikije0hPzH9MqhDBPxC61qtl2okpSfQLv3KPXxRsemgMGR7UfpaAb/c5sVBvB6w9UfYALw9EVmj6kWuElx+R/3Kl14Ohmmf7O1tJ4uNpIt/kz8x73b1eOruZ3fAhordrcmUzmPe5Wob9AubrqFeC7af+sSrjzepU4/iwcXqjqqotT9wphF5JE25uDEwz9WdVobfxITQ6SRLp0SUuAb9vCX0/bOxIhSodD88DBFeoXcIMaQJlqatEiMVLVN9+tXVtChBqJXK42DP62aIM0gsapVeb9f+R/zOl1aoOYg3Phr2NtDk5qSmL4Nji/veCPu7gXEsML1pXjVoFtIT0BYsPMf+w1lw7AhVD1fSjI97FyCzVRcce3ty9+bPtKrai3nVT4eESpIUl0cWB0gIHTIPgJNYxl2bNF2wEtipc1b6j2UEcWq8lSQojCy8mCw4tUCy5nT/MeG9gaBn0D57fAiufzX7DISlM9c3OyVAsxc69zK7+6ENhOrWzmtaIaf1ZN5yuupRw3ajEG3MqqjXUFdWSxehWgXl/zr3dtZb4ore5CfwEHF9VDu6DaPwdXL908WvtqlBrU0nSkGrsu7nmSRBcXBgP0+Z/aAb33N1j0eLGbzCMK4UyI6rvZaKjq4brnN3tHJETJdmqdGpbRZHjhHt9kOHR6Gfb9nncrM12H5c/Dpf1w/49QrlaRwv1X0DjVI/nsxtvvuzbqu1YJSKKd3KDNU3BytRoPfSemHDUB8PAi1bbPtYz51/OtoSY4FnZyYcZVVdPeaKh516/ZTXVh2fbV9Sc+O75Rryi0f7ZwsYhSR5Lo4kTToPub0P0tOLwAQqRXY4mWmQJLnwHfmmr1q1YPtRIlT46EKLxD81QyVJRV2y6vqtKCNW+p4Ro32vWjanHW5VWo27tosd6o/kDVEi2vDYan14N3oOp+URK0egycPNUGu1slXlALB/MfgU9qwk/d1IpuqwmFu5amqdXowq5EH5oPmcl331CY13XbPwexJ+DE36osb/fPqrtI2ZqFi0WUOpJEF0cdX1AbUbZMhYv77R2NKKz176nNT4Omqc1FrSZA8mUZsiNEYWVcheMrVQJclAEXBgMMnq5qXxc9pmpmAc5vg1WvQp0+arXakhxdoNlD6v//jT2Ic7LUBsZa3YpWd21Lrj7Qajwc/Ut16zi5Bv55FaYFwxcNYOlkVTNdt69qCzjlJNTpVfjrBbRWq/jJ0eY9TtdVKUf5xlAlyPzrNhisNq1u/RJ2/wiZV1WHEiFySRJdXPX+ANz9YMnEu2+AEcVPxC7YMV0lztd2g9fuqVabdv9k39iEKKmOr4DsNNUloqgcXWHEbLU6/OcIuLBHtcArUw3u/z7viXZF1XKc6kG8b9b12yJ3q+SsJNRD36jtRDW45LsO8McwCP0ZvCtDr/fgqe3w4nG1IbPxsDv3ZS6IwtZFX9gLlw8WfEPhrYwO0HayKiXZ/DnU6gkVm5h/HlFqSRJdXLmWgf5fqAb/eb1kJoqv7AxYMgm8q0CPt6/fbjCqX+bnNkP0cbuFJ4RF6Tps/Qr+eQ1ysq17rYPz1MrgtSEcReVZXrW+S0+EH7urDYUj/rTexLZytaBaR7Xv5Vqd7al1oBmhRmfrXNNaPPyh3+eqS8XDi+CVczB6MbSbDOUbWHZVvWJTMDqbXxcd+jM4ukPjBwp/7eYPqydaWanqVWIhbiBJdHFWry80GqaGCkQdsXc0oqA2faLaMfWfevuu/hZj1OpN6Ay7hCaERWUkq9XbNW+oTVfLnil6P9/8JEfDmQ0qIbLkKnGFxjBshlq4GPKd6qRhTUGPQkL49c2Ep9dBlVYlc9Ry81Fw3/tqQ6Sjq/Wu4+AMlVtCuBlt9dISVD/nJg+Ai1fhr+3kBr3eVa8iFKTHtLinSBJd3PX5WP1y/etp66/yiKK7fEi9ctB0ZN7TzdzLqXrO/bNVAiJESRV/Fmb0guPL1Uv4nf+j+iCvetU6ve4PL1QdbixRynGrun3gpdNqtLW11euvSvX2/AIpcWrfS81iOuq7OAlsrWrXM1MLdvzBuar0x9wNhXlp/jAMmFr084hSR5Lo4s69LPT7VLVbyqsdkyg+crJVDburL9z3Qf7HtZqgaiAPzrVdbEJY0pkQ+LErJEXCqAXqJfwu/1Ev7e/8Dja8b/lrHpynVo3961n+3GCdGui8ODip4Vphf+cOX9FLRms7ewtoo+rJL+69+7EpsarLSqUWUKmZ1UMT9y5JokuChkNUe6SQjyCmCFObhHVt+0qtlPT79M7jYKu0UsnA7hkyndKWLuyBk2tV/asoHF2H7d/CrPvBowI8tuF6AqhpakW6xRhV0rRlquWuG3daJU/WWIW2h5ZjQc9RTzZcy0Cl5vaOqPgLCFbv77S5UNfhwByY1gqunIPOFu6wIsQtHOwdgCigfp+pDWlLJsKjq9QmNVF8xJ5UT3LqD4AGg+58rKapPqvLnlF/EKq2tU2MxUn0MfAOAGcP21wv6RL82l9tDkID/wbqj3JgG7VJrUy1ktNezF6y0mH5c6qHcr3+qn741pp/TVN7ATJTYO1b4OQOwY8V/doH5wGa6vRQGvjWgBpdVY133T7y+7wg3HyhXN38NxdeOQfLnlP/pgGtYcBX1nvVQohcshJdUnj4q/royN2qdZooPkwm1RfV0RX6flawxzQeBs7e92a7u0sHVVuseaNttxK/4X3Vj3fYL2qIhmd5OLQAFj8BXzWDz+rC3NGwbZpqiyWvENws6SL80uf6EJLhs/IfhW0wwpDvVa/llVPUymBR6LoasFKtA3hVKtq5ipOgR9X7WnnsnRB5C2ytkugbN6+actT/22/bqr+PfT+Fcf9IAi1sQlaiS5LGD6jxqevfVasXMjWpeNj9k9o1Pni6Ss4KwsldDV7Y/RMkf6ieJN0LcrJgydPq49Pr4djSu6/cF9Xlw2rEc9uJ0Oj+67ebctSKeMQOCN+p3h9bqu7r/Ap0fc26cZUU4Tth7sNqFf/BP6B+/7s/xugID/wKfz6gNkU7uRd+096FvRB/BjqUsvZi9Qeodnq1izCE5F4T0EZNQ4wNA//6aiP30mdUqU+d3uoVW+8q9o5S3ENkJbok0TTo/7nql7l0svVaSYmCSwiHtW+rQQlNR5r32FbjwZSlesbeK7ZMVX/4hs5QU8T+eVW99G9Na95QHW46vnjz7QYjVGikNnoO/RGeOwQvHFffx43/Ux1U7nWn1sKv/VQSPGFtwRLoaxxd1DCTyi1gwaOqH3JhHJqnfuc1GFi4xxdXmgb1+qknHKJgrg1dORMC6/4PfuiifgcP+xlGzpEEWticJNEljVclNc3w/FbpNVwcrH9PvR8w1fya2nK1oUYXCP313mhfGHVUJacN74eGg9UGzKQLahOatZxaq1a8O79y582e13hVVLWU1TqqJ6rntlovtuLuyjlYMF71TX5svVr5M5ezB4yaD+XqwJxRahS0OXKyVWu7OveVzD7KwrJ8a6j2gP+8Cps/gyYPwqTd0Gio7GkQdiFJdEnUbJRa+VzzlvpDJ+wj6ZIqr2kxRk1RK4xWE1SbsJOrLBtbcZOTrco4XLyhb27SHNhG/SxvmwYxJyx/TVMOrH4DylRX/84F5eAED84C3+owd5TqDHGvyUpTNeK6rv4tCvIEJD+uZdQkO+/K8OdwiNxT8MeeCYGUGGhSSrpyiKLRNFUG41tD/UwN/rZoP5tCFJEk0SWRpsGAL0Ez5JZ15Ng7ontT6AzVt7T144U/R50+4Fmp9G8w3P41XNynEmj3ctdv7/EOOLrB3y9ZfjPfvt8h+ij0fEclxuZwLQMPzVP/x/4YBqnxhYsh8YKatFeS6DqseBEuH4T7f1AJS1F5+MOYJeDqA7/2ze22UQCH5qknXlI3LK7p/wU8s1cG1IhiQZLoksonAHp/CGc3wYoXil83gZNr1MttoT/DkcVqRenSAVW/lnG1+MVrrqx09bXV7VO0JMPoAEHjVMlBaV3xjDkBGz5UbdEaDrn5Pg8/6P6G+vk4+pflrpmRrDpyBLRRPdYLw7e6qulNvKBKEbIzCv5YU46q//6qOfx8n1rZLarIUFg4Qf3/saY9v6ohIJ1ehrq9LXde7yqqr3TlIFj0GKx6/c5lTJkpcGy52njq4Gy5OIQQwkKkO0dJ1mK02rW+5XM1+KDrq/aOSA2y+PsV1QrrTgyOarXP3U89GajR2TbxWcqh+ZAaB22eKvq5WoxVtcK7Z6h699LElKN6mzu5Qb/P865bDHpU7bj/5zWo1dMyvaO3fQXJUar7QVFqJQNbq5eMF45Xr/oM+f7u54sJUx0pLoRC1Q5wfov6/vZ4u/BxZKaoGK6cA7960GlK4c91J5F74O+XVdu1Lv+x/Pndy8GYv2DVa7B9GkQdUZvC8npJPuxvyEopPQNWhBCljiTRJV33N9XLxRs/Ui+Zthpvv1jObYHFT6qesp1fgXbPQGYypF1RL4enXbnlLV6twC5+Eibtyr/vbHGj66pXd/lGagNaUXmWV6ul+3+Hbv9VCWdpsfM7iNwFQ37Iv/2fwahaU83oCZs+hp7/V7RrJl2ErV+pDYxVgop2LlA9vePPwob3wLcmdHkl7+NyslXZyoYPVTeLoTPUhqclE1U8jYaqSZWFse7d3AS6Pmz7Wg0wsfRGu5RY1bvbswLc/6P1BoAYHVVZT4Um6lW0H7uqJzvlG9583MF54FUZqra3ThxCCFFEUs5R0l2rj67TW9UxHl1i+xiyM2D1f9VEOKMTjF+teuw6e6g/yP71oVp71R6rxWho/wz0eEvFPfRnuHoJ1r9v+7gL69xmiD4CrZ+03I7wVhPUKv7hhZY5X3EQd1olf3V6331jWEAwNH8Ytn9T9NH2699XI5V7vFW089yo0xRo+hCEfAAH599+f/Rx+LmXandYpxdM3KmS72ujsN18C79/IXyHejIS/DgMmQ7pCbDz+6J+RTfLyYYF49SrK8OLuJGwoFqMhkdWqtKon3re/LsrJQ5Or1P/hgb5MyWEKJ7kt1NpYHRQk9iqtFI1k+e22O7alw/DD13V6ljQOHhys3mrfwGt1Or5ru/VxrOSYMd0cCurht9YStV2apVx948lv14crk9xNDqpjUAFebLR4x21grtySuH/DS4fUvW8rZ9Qo7wt5dqT1aodVJeRa63acrJh8+fwfUe1Wj3sZ5WE3jg8x80Xen+kfr53fmfedbPS1Eq2TwB0fwsqNYe6/VQpRFqCxb481r+r9lf0+xwqNbPcee8moBU8HgLlG8C8MaplpMkERxapTbtSyiGEKMYkiS4tnNzgobmqndfskSqZsCZTDmz9Ur0UmxoLD81XyZKTu/nn6v6mqo1e9mzx75ccf0bVagY9qoZJWIqmQfAEtfkycrflzmsvoTNUL/P73i/4qGb3cupn4ewmlUSZS9fVKyKuPrcPVrGEa63vvANgzkMQ9o8qQVn3jlptn7gr/361jYaqDhPr34Mr5wt+zQ0fQNwpGPj19VrxLv9Rr1rsmG6Zr+voUtg6FVqOg+ajLHNOc3hVhEdWqFciNn0Cc0aqzir+DdQwHCGEKKYkiS5N3Hxh9CJVW/z7UOv1kL5yHn4bAGveVEMQntquXsIuLBdvtVJ36YBaiS3Odv4ABgcIskLteZMH1Qr3hhJU2pKXK+dUD/Oa3VViZI6W46BiU9W5wdwuFKfWqi4fnf+jNq1ag5uvGh6CDrMfhITz6lWg4TNVp5H8aJpa5UUreDedyD1qxbnlI2oozzUVm6hOJzu+VXsLiiL2pNoEWbkl9Plf0c5VFA7OMHAa9P1UfR8v7bfsKz1CCGEFkkSXNt5V4OGFqk551v1qs5Alhe+E6e3h0kEY/J166dq9bNHP23CI6syw/j1IjCz6+awhPUmtkDUcolbPLM3ZEzpOUYng6Q2WP78t6DosfeaGXuZm1owbjCrZvHpJdbQoqJxstQrtW0O9SmBNZWvCqIXQdhI8vRMa3V+wr9MnQK20n1oLhxbc+djsDFU24lkx742WXV6FjCRVQ15YGckw92G1wj58pv3byGma2jA5ZokaqGHuEzAhhLAxSaJLI//6alBE0kX44wH1x9JS1r0DLl7w1FZoNtJyG+s0TY2BNuWoFnnF0f4/IPOqZdra5afVeFUusO6dklcbreuqxOfsRpX4+QQU7jxVgtQUyB3TIfpYwR6zbxbEHFfXNXewSmFUaalKVe60+pyX4MdUn+R/XlGb5/Kz8WP19Qz4Ku8uHBUaqf7JO74r3CAYXYelkyD2hKrj9q5i/jmspVoHePD3m+vKhRCiGJIkurQKbA0P/KJKJOaNhuzMop8zYreqc207CcpULfr5blWmmqr3PL5cDVkoTkw5alNYQGuo3MJ613FwVp1NLu6zT6eVwkqOUQNJ1r4Fdfuqsoyi6P42OHnAiinqZzj6mOr2kRChWjqmXYHMVLUCnXFVlcAEtlVlDsWZwQgDv1I1zatfz/uYSwdgyxdqJHrtHvmfq/N/VAvJbV+bH8e6/1NDkLq/dXOpiBBCiALT9JK22gUEBQXpoaGh9g6jZNg7S604tXpMrfQWxZxRqvPH80csMxAjLzlZ8H1n1cZr4s7i0zv6+Eq14emBX2+fumdpphxVMmPKUuUCxmLezj3sb9WJIz1RJWVtnrZMW7LQn2H58wU/fsJ6tUJcEqz7PzXRc/Tim8cXZ2fCj90gJQYm7rh7bff8cXBiFTx3qOBlVbt+VB1QWj4C/9/enUdZVV15HP9uRpkMIIQYBqEVUeyIAkERCSAok8FhiYpEIhloAjh0YtS0rsTE2BIVgwNiEyVi20hntQMa6I7YiqKMmgYUECVogKgoiIoYxara/cd+LsqiCt6tN1bV77NWLXnv3ffuce0a9j33nL3PnJ69u0kiIrWUmb3k7vuVHivyv86SsZ4XR6WOVb+LW8ltu1Xvc3a8Dq8uiHq5uUqgIRoxfHt6VD145qbi6eC3/G44tAMc8+3cn6te/Vg7O29MNGDpdUnuzvXFDHurzlE9on7D9N/72cfRee7Pc6LxzLj5+zfMyESv8fG5e96LNcKln0PpZ1C6N5LN0nJfrY+sOQk0REvt9fPhiStg0vJ9DXZemA7bX4524+lsjhx4TcwoL709vSY1G/4IC38KRw+HEdOUQIuIZEBJdF0w4CpYPTdqwV7wYPU+Y+mdUfO3zz9ld2yV6dgnNoetmBlNOvJZt7Yy77wSDVaG/DJ/s8LdhkOHPrB4alTtaNgkN+d55l9hSeoORbO2ca4Txkbd3gPZsgIenRCVWvpdEUtQsr0xzSy+F2qjhofExsv7R0YDlzN+HS2wn70Z/vE8OGZEep/Ttls0JFn5O+h76YHXaG9ZEa3D2/eKddDFfodDRKTIaU10XdCsDZxyKWx4ArZVYxnM7u2w5qGoIZt0I1V1Df4FNG0TtaOr0+Utm1bMhIZNY7NbvpjBkOujSsXKWbk5x/rHI4E+8TswZl6s915xD8zsC7MGRmJWsYRayd7oQvj7YeBlMH4hnP7Lwld2qIk6nxrfU8tmRDm7+ZOjxvXwm5N9zoCroeTTmMWuyo7XoyTfoe2jnnxtai0vIlIgSqLrir6TY6bxqeuTV31YcU/cSu87JSdDq1STljB8atSLXVnA2tF7dkSb5x4X5qcVcnmd+8USiyW3Zbc7HcRGvUcnRqWIkbfFzPeF/wE/2QhDb4p4L7wSbu0W6243PQXb18N9QyLx7nERTHwhOi1K9Z3+q7hYfGBUbCYdcWvykpFtukZnv1X3xQVvRbvfgQfPjfrm33k4LqpFRCRjSqLrisbNYx3mm0vgL/+b/vs+/Sj+OHcfFfVx8+m4c6Nhx9M3wId/y++5v/Di72Md7kkTC3P+wT+PTZYv3J69z/z7rui417h5dOArP4vcrA30nQQTn4cJz0Kv78LmZ6J5z8y+UcP7ggfh7BlR6lAy06QVjLglqmwcOwqOO7t6nzPgqlgbXvH75LPdUeZyz84oe9m6S8ZDFhGRoCS6Lul1CbQ8Imajy8rSe8+f58BnH0K/y3M5ssqZwchpUFYSm6HyvayjZG9syDxqSPU3ZGbqa9+Izm3LZ8aMYqbKSuHhH0apuPMfqLolt1msRR9xS8xOj54TF2E/WhaNMCR7up8VmzLPzqCN92FHxt2SF+/b931Sshf+MC7WWp8/J7elGUVE6qCMkmgza21mi8zs9dR/K91ObmbDzGyjmW0ys2vKPX+Lmb1qZmvN7FEza5nJeOQgGjSC066Lah3rHjn48SV7Ydnd0Ll/bEYqhNZdogLBxgUwow+8dH9UasiH9Y/Bx9vhpBw2V0nHoH+JcndJOvhV5ZkbYdOiaPHc6eT03tOgccyQnnYttGiX+Rjky8yiVnOmVW++dWUsw3n+t7Fk64nL4C9PR13qrqdnZagiIrJPRnWizexm4H13n5pKjlu5+9UVjqkPvAacDmwDVgFj3H29mZ0BPO3uJWb2G4CK76+M6kRnoKwM/q1/3D6evOrA3d1Wz4XHfhQtjg/U9CHX3CPpf346vLMWmreLroG9v1d5N7eq7Hoz6j2//mTMbjdtHetRmx4WyxiaHrbvq1mbWPKwd0/Uas5G3eNMLLgy6iZPWVX9ZTXr58fMZM9x0QlP5c1qn/mTYw3/CWPignPQtbHUQ0REqq2qOtGZJtEbgYHu/raZHQ4sdvduFY7pC1zv7kNTj38G4O43VTjuHOA8dx97sPMqic7Qa0/C3NGxianPDys/pqwMZp4CVi9afBdDwuUOmxdHFYLNi6FRC+g9Ppp7HHp45ce/vSbqW29cCNtfiefbHgNNWsMnO+CTnam2yVX8HIycBt/8QW7+f5LYvR3uOCE2AJ43O/n7t6+He4dE6bpLFqiaRm216024s1dcJKqZiohIVuSq2Uo7d38bIJVIf7WSY9oDW8s93gacVMlx3wP+s6oTmdkEYAJAp06dqj1gIW7tHtEvatL2GFP5beRNi+C9DXDOrOL5I2wGRw6Kr7dWxyaqZXdF9ZDjL4h12606R1fFVxdEJ72PtsWFQKe+cMaNUX+39T98+XPLSqP6xSc79yXWe3bErfET81jW7kBatIuLhSW3wimXJaudXX4j4fn/rgS6NmvVOWafP/irmqmIiOTYQWeizewp4GuVvHQtMMfdW5Y7dpe7f2ldtJmNBoa6+w9Sjy8G+rj7peWOuRboDZzraUyNayY6C7aujK6Ag66DAT/d//XZw+GDLXD56mRd7PLt/Tcikf6/B2OtdKPmsHc3NGgCRw2GbiPg6GHJy4YVo08/hNt7wNd7wsVprGmHuECYez5sfjZmoDtVdv0qIiIiVan2TLS7V7kY1sy2m9nh5ZZzvFvJYduAjuUedwDeKvcZ3wXOBAank0BLlnTsA91GwtI7Ym1x+SRz60rYsjTqBRdzAg2x8XDkNBj4s6gn/fE70HVobNSqbQ0lDvkK9P8JPHkdvPEcdPnWwd/z9K+jxvOZv1UCLSIikkWZrom+BdhZbmNha3e/qsIxDYiNhYOBvxEbCy9y93VmNgy4DRjg7u+le17NRGfJuxti3fPJk2Dojfuenzc2lkT887rMKwZIdn3+KdzZMzooHjU4GmjUqw9WP/XvBrEJsl6DWMbxwu3aSCgiIpKBXK2Jngr8wcy+D2wBRqdO9nXgXncfkaq8MQX4E1AfmO3u61LvvwtoDCyy+AO/3N0L1NWiDvrqsbEmeuWsaCbSsmO0B351QZTLUgJdfBoeErWb//jjaMVeVhqbyMpKowxeRZ37xwZSJdAiIiJZldFMdKFoJjqLPtgau/m/MTq60D1+GayZF7PQzdsWenSSVFkZeCqxLv0cGrdQAi0iIpKBqmai1bGwrmvZMcrcrZkbm8/WPAQnjlUCXVPVqxfr2Bs2ibbcSqBFRERyQkm0wKk/jqoWcy+I2cu+Uwo9IhEREZGipiRaojLHKZdByd+h+6jqd8QTERERqSMy3VgotUXfSfDBm9DvikKPRERERKToKYmW0KgZnDWj0KMQERERqRG0nENEREREJCEl0SIiIiIiCSmJFhERERFJSEm0iIiIiEhCSqJFRERERBJSEi0iIiIikpCSaBERERGRhJREi4iIiIgkpCRaRERERCQhJdEiIiIiIgkpiRYRERERSUhJtIiIiIhIQkqiRUREREQSMncv9BgSM7P3gL8W4NRtgB0FOK/kl+JcNyjOdYPiXPspxnVDIeN8hLu3rfhkjUyiC8XMXnT33oUeh+SW4lw3KM51g+Jc+ynGdUMxxlnLOUREREREElISLSIiIiKSkJLoZGYVegCSF4pz3aA41w2Kc+2nGNcNRRdnrYkWEREREUlIM9EiIiIiIgkpia6EmQ0zs41mtsnMrqnkdTOzO1KvrzWznoUYp2QmjTiPTcV3rZktNbMehRinVN/BYlzuuG+aWamZnZfP8Ul2pBNnMxtoZqvNbJ2ZPZvvMUrm0vid/RUze8LM1qTiPL4Q45TqM7PZZvaumb1SxetFlX8pia7AzOoDM4DhQHdgjJl1r3DYcKBr6msCMDOvg5SMpRnnN4AB7n48cANFuB5LqpZmjL847jfAn/I7QsmGdOJsZi2Bu4FR7n4cMDrf45TMpPnzPBlY7+49gIHANDNrlNeBSqbuB4Yd4PWiyr+URO+vD7DJ3Te7+15gHnBWhWPOAh7wsBxoaWaH53ugkpGDxtndl7r7rtTD5UCHPI9RMpPOzzLApcDDwLv5HJxkTTpxvgh4xN23ALi7Yl3zpBNnB1qYmQHNgfeBkvwOUzLh7s8RcatKUeVfSqL31x7YWu7xttRzSY+R4pY0ht8H/junI5JsO2iMzaw9cA5wTx7HJdmVzs/y0UArM1tsZi+Z2bi8jU6yJZ043wUcC7wFvAxc7u5l+Rme5ElR5V8NCnXiImaVPFexhEk6x0hxSzuGZjaISKJPzemIJNvSifF04Gp3L43JK6mB0olzA6AXMBhoAiwzs+Xu/lquBydZk06chwKrgdOAI4FFZrbE3T/K8dgkf4oq/1ISvb9tQMdyjzsQV7VJj5HillYMzex44F5guLvvzNPYJDvSiXFvYF4qgW4DjDCzEnd/LC8jlGxI93f2DnffA+wxs+eAHoCS6JojnTiPB6Z61O7dZGZvAMcAK/MzRMmDosq/tJxjf6uArmbWJbUh4ULg8QrHPA6MS+0SPRn40N3fzvdAJSMHjbOZdQIeAS7WjFWNdNAYu3sXd+/s7p2B/wImKYGucdL5nT0f6G9mDcysKXASsCHP45TMpBPnLcTdBsysHdAN2JzXUUquFVX+pZnoCty9xMymEDv16wOz3X2dmU1MvX4PsBAYAWwCPiGufqUGSTPOPwcOA+5OzVSWuHvvQo1ZkkkzxlLDpRNnd99gZv8DrAXKgHvdvdISWlKc0vx5vgG438xeJm77X+3uOwo2aEnMzB4iKqu0MbNtwC+AhlCc+Zc6FoqIiIiIJKTlHCIiIiIiCSmJFhERERFJSEm0iIiIiEhCSqJFRERERBJSEi0iIiIikpCSaBERERGRhJREi4iIiIgkpCRaRERERCSh/wdZNTsyOMbe/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_pred, y_targ = test_generator[0]\n",
    "X_pred = X_pred[0]\n",
    "y_targ = y_targ[0]\n",
    "y_pred = model.predict(np.expand_dims(X_pred,axis=0))\n",
    "print(np.max(y_pred))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(foil['y'].x, y_targ), plt.plot(foil['y'].x, y_pred[0,])#, plt.plot([0,0],[-0.3,0.3])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 32, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10805403181651126\n",
      "0.09349667875105727\n",
      "0.09349381105285037\n",
      "0.09035233992894652\n",
      "0.07863152720333667\n",
      "0.119013378313855\n",
      "0.05892802179134889\n",
      "0.05939866890947117\n",
      "0.047171300384526174\n",
      "0.06048534209968928\n",
      "0.05188243859065457\n",
      "0.054179565161679114\n",
      "0.0900383017473756\n",
      "0.09868313198702476\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(test_generator):\n",
    "    a, b = test_generator[i]\n",
    "    print(np.max(b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(str(Path(\"./weights\", 'initial_weights_1000foils_04122020.h5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_unet_1(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   \n",
    "    inputs = Input(shape=input_shape)\n",
    "    bn1 = BatchNormalization()(inputs)\n",
    "    conv1 = Convolution2D(16, (2,2), activation='tanh', padding='same')(bn1)\n",
    "    conv1 = Convolution2D(16, (2,2), activation='tanh', padding='same')(conv1)\n",
    "    bn2 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(bn2)\n",
    "    \n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(conv2)\n",
    "    bn3 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(bn3)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(conv3)\n",
    "    bn4 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(bn4)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(conv4)\n",
    "    bn5 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(bn5)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(conv5)\n",
    "    bn6 = BatchNormalization()(conv5)\n",
    "    up6 = Concatenate()([Convolution2D(256, (2, 2), activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(bn6)), conv4])\n",
    "    conv6 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(up6)\n",
    "    conv6 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(conv6)\n",
    "    bn7 = BatchNormalization()(conv6)\n",
    "    up7 = Concatenate()([Convolution2D(128, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(bn7)), conv3])\n",
    "    conv7 = Convolution2D(512, (2,2), activation='tanh', padding='same')(up7)\n",
    "    conv7 = Convolution2D(512, (2,2), activation='tanh', padding='same')(conv7)\n",
    "    bn8 = BatchNormalization()(conv7)\n",
    "    up8 = Concatenate()([Convolution2D(64, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(bn8)), conv2])\n",
    "    conv8 = Convolution2D(256, (2,2), activation='tanh', padding='same')(up8)\n",
    "    conv8 = Convolution2D(256, (2,2), activation='tanh', padding='same')(conv8)\n",
    "    bn9 = BatchNormalization()(conv8)\n",
    "    up9 = Concatenate()([Convolution2D(32, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(bn9)), conv1])\n",
    "    conv9 = Convolution2D(16, (2,2), activation='tanh', padding='same')(up9)\n",
    "    conv9 = Convolution2D(16, (2,2), activation='tanh', padding='same')(conv9)\n",
    "    \n",
    "    bn10 = BatchNormalization()(conv9)\n",
    "    \n",
    "    conv10 = Convolution2D(1, (1, 1), activation='tanh')(bn10)\n",
    "    \n",
    "    flat = Flatten()(conv10)\n",
    "    \n",
    "    dense = Dense(128, activation='tanh')(flat)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "        model.layers[layer].kernel_initializer=1e-12\n",
    "        model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_unet_2(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   # no bn\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    conv1 = Convolution2D(16, (2,2), activation='tanh', padding='same')(inputs)\n",
    "    conv1 = Convolution2D(16, (2,2), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(conv5)\n",
    "\n",
    "    up6 = Concatenate()([Convolution2D(256, (2, 2), activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv5)), conv4])\n",
    "    conv6 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(up6)\n",
    "    conv6 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(conv6)\n",
    "\n",
    "    up7 = Concatenate()([Convolution2D(128, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv6)), conv3])\n",
    "    conv7 = Convolution2D(512, (2,2), activation='tanh', padding='same')(up7)\n",
    "    conv7 = Convolution2D(512, (2,2), activation='tanh', padding='same')(conv7)\n",
    "    \n",
    "    up8 = Concatenate()([Convolution2D(64, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv7)), conv2])\n",
    "    conv8 = Convolution2D(256, (2,2), activation='tanh', padding='same')(up8)\n",
    "    conv8 = Convolution2D(256, (2,2), activation='tanh', padding='same')(conv8)\n",
    "    \n",
    "    up9 = Concatenate()([Convolution2D(32, (2, 2),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv8)), conv1])\n",
    "    conv9 = Convolution2D(16, (2,2), activation='tanh', padding='same')(up9)\n",
    "    conv9 = Convolution2D(16, (2,2), activation='tanh', padding='same')(conv9)\n",
    "    \n",
    "    conv10 = Convolution2D(1, (1, 1), activation='tanh')(conv9)\n",
    "\n",
    "    flat = Flatten()(conv10)\n",
    "    \n",
    "    dense = Dense(128, activation='tanh')(flat)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "        model.layers[layer].kernel_initializer=1e-12\n",
    "        model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_unet_3(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   # contracting path only\n",
    "    inputs = Input(shape=input_shape)\n",
    "       \n",
    "    conv1 = Convolution2D(16, (2,2), activation='tanh', padding='same')(inputs)\n",
    "    conv1 = Convolution2D(16, (2,2), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(256, (2,2), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (2,2), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (2,2), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(conv5)\n",
    "\n",
    "    flat = Flatten()(conv5)\n",
    "    \n",
    "    dense = Dense(128, activation='tanh')(flat)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "        model.layers[layer].kernel_initializer=1e-12\n",
    "        model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def define_unet_4(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   # no bn\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    conv1 = Convolution2D(16, (3,3), activation='tanh', padding='same')(inputs)\n",
    "    conv1 = Convolution2D(16, (3,3), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(256, (3,3), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(256, (3,3), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (3,3), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (3,3), activation='tanh', padding='same')(conv5)\n",
    "\n",
    "    up6 = Concatenate()([Convolution2D(256, (3,3), activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv5)), conv4])\n",
    "    conv6 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(up6)\n",
    "    conv6 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(conv6)\n",
    "\n",
    "    up7 = Concatenate()([Convolution2D(128, (3,3) ,activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv6)), conv3])\n",
    "    conv7 = Convolution2D(512, (3,3), activation='tanh', padding='same')(up7)\n",
    "    conv7 = Convolution2D(512, (3,3), activation='tanh', padding='same')(conv7)\n",
    "    \n",
    "    up8 = Concatenate()([Convolution2D(64, (3,3),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv7)), conv2])\n",
    "    conv8 = Convolution2D(256, (3,3), activation='tanh', padding='same')(up8)\n",
    "    conv8 = Convolution2D(256, (3,3), activation='tanh', padding='same')(conv8)\n",
    "    \n",
    "    up9 = Concatenate()([Convolution2D(32, (3,3),activation='tanh', padding='same')(UpSampling2D(size=(2, 2))(conv8)), conv1])\n",
    "    conv9 = Convolution2D(16, (3,3), activation='tanh', padding='same')(up9)\n",
    "    conv9 = Convolution2D(16, (3,3), activation='tanh', padding='same')(conv9)\n",
    "    \n",
    "    conv10 = Convolution2D(1, (1, 1), activation='tanh')(conv9)\n",
    "\n",
    "    flat = Flatten()(conv10)\n",
    "    \n",
    "    dense = Dense(128, activation='tanh')(flat)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "        model.layers[layer].kernel_initializer=1e-12\n",
    "        model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_unet_5(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   # contracting path only\n",
    "    inputs = Input(shape=input_shape)\n",
    "       \n",
    "    conv1 = Convolution2D(16, (3,3), activation='tanh', padding='same')(inputs)\n",
    "    conv1 = Convolution2D(16, (3,3), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(256, (3,3), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(256, (3,3), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (3,3), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (1,1), activation='tanh', padding='same')(conv5)\n",
    "\n",
    "    flat = Flatten()(conv5)\n",
    "    \n",
    "    dense = Dense(128, activation='tanh')(flat)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "        model.layers[layer].kernel_initializer=1e-12\n",
    "        model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_unet_6(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   # contracting path only\n",
    "    inputs = Input(shape=input_shape)\n",
    "       \n",
    "    conv1 = Convolution2D(16, (3,3), activation='tanh', padding='same')(inputs)\n",
    "    conv1 = Convolution2D(16, (3,3), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(256, (3,3), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(256, (3,3), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (3,3), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (1,1), activation='tanh', padding='same')(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "    \n",
    "    conv6 = Convolution2D(512, (3,3), activation='tanh', padding='same')(pool5)\n",
    "    conv6 = Convolution2D(128, (1,1), activation='tanh', padding='same')(conv6)\n",
    "    \n",
    "    flat = Flatten()(conv5)\n",
    "    \n",
    "    dense = Dense(128, activation='tanh')(flat)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "        model.layers[layer].kernel_initializer=1e-12\n",
    "        model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_unet_7(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   # contracting path only\n",
    "    inputs = Input(shape=input_shape)\n",
    "       \n",
    "    conv1 = Convolution2D(16, (4,4), activation='tanh', padding='same')(inputs)\n",
    "    conv1 = Convolution2D(16, (4,4), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(64, (3,3), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(64, (3,3), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "    \n",
    "    conv6 = Convolution2D(512, (1,1), activation='tanh', padding='same')(pool5)\n",
    "    conv6 = Convolution2D(128, (1,1), activation='tanh', padding='same')(conv6)\n",
    "    \n",
    "    flat = Flatten()(conv5)\n",
    "    \n",
    "    dense = Dense(128, activation='tanh')(flat)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "        model.layers[layer].kernel_initializer=1e-12\n",
    "        model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def define_unet_8(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "   # contracting path only\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    bn1 = BatchNormalization()(inputs)\n",
    "\n",
    "    conv1 = Convolution2D(64, (4,4), activation='tanh', padding='same')(bn1)   \n",
    "    conv1 = Convolution2D(64, (4,4), activation='tanh', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(4, 4))(conv1)\n",
    "    \n",
    "    conv2 = Convolution2D(128, (3,3), activation='tanh', padding='same')(pool1)\n",
    "    conv2 = Convolution2D(128, (3,3), activation='tanh', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(pool2)\n",
    "    conv3 = Convolution2D(512, (3,3), activation='tanh', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(pool3)\n",
    "    conv4 = Convolution2D(1024, (3,3), activation='tanh', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(pool4)\n",
    "    conv5 = Convolution2D(2048, (2,2), activation='tanh', padding='same')(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "    \n",
    "#     conv6 = Convolution2D(512, (1,1), activation='tanh', padding='same')(pool5)\n",
    "#     conv6 = Convolution2D(512, (1,1), activation='tanh', padding='same')(conv6)\n",
    "    \n",
    "    flat = Flatten()(conv5)\n",
    "           \n",
    "    dense1 = Dense(2048, activation='tanh')(flat)\n",
    "    dense2 = Dense(1024, activation='tanh')(dense1)\n",
    "    dense3 = Dense(512, activation='tanh')(dense2)\n",
    "    dense4 = Dense(256, activation='tanh')(dense3)   \n",
    "    dense5 = Dense(128, activation='tanh')(dense4)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense5)\n",
    "\n",
    "    for layer in range(len(model.layers)):\n",
    "        model.layers[layer].kernel_regularizer=l2(reg)\n",
    "        model.layers[layer].bias_regularizer=l2(reg)\n",
    "#         model.layers[layer].kernel_initializer=1e-12\n",
    "#         model.layers[layer].bias_initializer=1e-12\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "\n",
    "    print('U-Net compiled')\n",
    "    print('Input shape:', model.input_shape)\n",
    "    print('Output shape:', model.output_shape)\n",
    "\n",
    "\n",
    "def define_resnet(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "\n",
    "    base_model = resnet50.ResNet50(weights= None, include_top=False, input_shape= input_shape)\n",
    "    x = base_model.output\n",
    "    flat = Flatten()(x)           \n",
    "    dense1 = Dense(4096, activation='tanh')(flat)\n",
    "    dense2 = Dense(1024, activation='tanh')(dense1)\n",
    "    dense3 = Dense(128, activation='tanh')(dense2)\n",
    "    \n",
    "    model = Model(inputs = base_model.input, outputs = dense3)\n",
    "    \n",
    "    for i, _ in enumerate(model.layers):\n",
    "        model.layers[i].kernel_regularizer=l2(reg)\n",
    "        model.layers[i].bias_regularizer=l2(reg)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def define_fcnn(input_shape=(64, 64, 1), optimizer=Adam(1e-5), lr=1e-5, reg=0):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    flat = Flatten()(inputs)\n",
    "    \n",
    "    bn1 = BatchNormalization()(flat)\n",
    "           \n",
    "    dense1 = Dense(2048, activation='tanh')(bn1)\n",
    "    dense2 = Dense(1024, activation='tanh')(dense1)\n",
    "    dense3 = Dense(512, activation='tanh')(dense2)\n",
    "    dense4 = Dense(256, activation='tanh')(dense3)   \n",
    "    dense5 = Dense(128, activation='tanh')(dense4)\n",
    "    \n",
    "#     for i, _ in enumerate(model.layers):\n",
    "#         model.layers[i].kernel_regularizer=l2(reg)\n",
    "#         model.layers[i].bias_regularizer=l2(reg)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# model = define_unet(input_shape=(64,64,1), optimizer=optimizer, lr=lr, reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = y[0, :16].reshape((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_y=np.expand_dims(foil['y'].y,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_y = normalize(my_y,norm='l2', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(my_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05351338, 0.18805768, 0.31627041, 0.44215853],\n",
       "       [0.18722034, 0.22907079, 0.27095795, 0.31275092],\n",
       "       [0.21307967, 0.23785892, 0.26240639, 0.28665502],\n",
       "       [0.2246236 , 0.24170002, 0.25853736, 0.27513902]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty = normalize(t,axis=1,norm='l1')\n",
    "ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6784369803131107"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ty[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000150024654466405"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выстрелила model5 - только contracting path от Unet\n",
    "\n",
    "### Как получил начальные веса\n",
    "\n",
    "#### Шаг 1 - тренировка на 138 профилях группы ['ag', 'cl', 'mh', 'hq', 'hd', 'hn', 'ht' ]\n",
    "138 штук\n",
    "n_epochs=10000\n",
    "batch_size=1\n",
    "lr=1e-4\n",
    "reg=0.1\n",
    "optimizer=Adam(learning_rate=lr)\n",
    "model5\n",
    "\n",
    "Результат сохранен в 'initial_weights_138foils_04122020.h5'- для профилей группы ['ag', 'cl', 'mh', 'hq', 'hd', 'hn', 'ht' ], \n",
    "\n",
    "#### шаг 2 - 300 первых профилей по списку\n",
    "подгрузил веса с предыдущего шага\n",
    "n_epochs=1000\n",
    "batch_size=1\n",
    "lr=1e-5\n",
    "reg=0.1\n",
    "\n",
    "val_loss = 7.844471713522379e-05\n",
    "\n",
    "Результат в initial_weights_300foils_04122020.h5\n",
    "\n",
    "#### шаг 3 - 600 первых профилей по списку\n",
    "подгрузил веса с предыдущего шага\n",
    "n_epochs=1000\n",
    "batch_size=1\n",
    "lr=1e-5\n",
    "reg=0.1\n",
    "\n",
    "val_loss = 4.179015103318306e-05\n",
    "\n",
    "Результат в initial_weights_600foils_04122020.h5\n",
    "\n",
    "#### шаг 3 - 1000 первых профилей по списку\n",
    "подгрузил веса с предыдущего шага\n",
    "n_epochs=1000\n",
    "batch_size=1\n",
    "lr=1e-5\n",
    "reg=0.1\n",
    "\n",
    "val_loss = 6.332580076332306e-05\n",
    "\n",
    "Результат в initial_weights_1000foils_04122020.h5\n",
    "\n",
    "\n",
    "\n",
    "##### TODO\n",
    "- интересно, а с нуля сможет зохавать все профили или упадёт? падает\n",
    "- определить наилучший batch_size - 1\n",
    "- добавить фильтров 4х4\n",
    "- добавить еще пару сверточных - блоков получше\n",
    "- уменьшить число каналов\n",
    "- попробовать другие активации\n",
    "- другой лосс?\n",
    "- регуляризацию подобрать, вроде больше-лучше 0.1...0.001 самое то\n",
    "- SGD плохо\n",
    "\n",
    "В итоге лучше всего Unet6 c :\n",
    "batch_size=1\n",
    "lr=1e-5\n",
    "reg=0.01\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4x4 пробуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model = define_unet_5(input_shape=(64,64,1), optimizer=Adam(0))\n",
    "# model.load_weights(str(Path(\"./weights\", 'initial_weights_1000foils_04122020.h5')))\n",
    "\n",
    "foil = load_pkl(Path(foils_pkl_path, 'ag14.pkl'))\n",
    "X_pred = foil['X'].reshape(64,64,1)\n",
    "y_targ = foil['y'].y\n",
    "y_pred = model.predict(np.expand_dims(X_pred,axis=0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(foil['y'].x, foil['y'].y), plt.plot(foil['y'].x, y_pred[0,])#, plt.plot([0,0],[-0.3,0.3])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-32c500991385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(history.history['loss'])), history.history['loss']), plt.plot(range(len(history.history['val_loss'])), history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg=0.0\n",
    "\n",
    "foil = load_pkl(Path(foils_pkl_path, 'ag14.pkl'))\n",
    "X_pred = foil['X'].reshape(64,64,1)\n",
    "y_targ = foil['y'].y\n",
    "y_pred = model.predict(np.expand_dims(X_pred,axis=0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(foil['y'].x, foil['y'].y), plt.plot(foil['y'].x, y_pred[0,])#, plt.plot([0,0],[-0.3,0.3])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg=0.1\n",
    "\n",
    "foil = load_pkl(Path(foils_pkl_path, 'ag14.pkl'))\n",
    "X_pred = foil['X'].reshape(64,64,1)\n",
    "y_targ = foil['y'].y\n",
    "y_pred = model.predict(np.expand_dims(X_pred,axis=0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(foil['y'].x, foil['y'].y), plt.plot(foil['y'].x, y_pred[0,])#, plt.plot([0,0],[-0.3,0.3])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg=0.01\n",
    "\n",
    "foil = load_pkl(Path(foils_pkl_path, 'ag14.pkl'))\n",
    "X_pred = foil['X'].reshape(64,64,1)\n",
    "y_targ = foil['y'].y\n",
    "y_pred = model.predict(np.expand_dims(X_pred,axis=0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(foil['y'].x, foil['y'].y), plt.plot(foil['y'].x, y_pred[0,])#, plt.plot([0,0],[-0.3,0.3])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg=0.001\n",
    "\n",
    "foil = load_pkl(Path(foils_pkl_path, 'ag14.pkl'))\n",
    "X_pred = foil['X'].reshape(64,64,1)\n",
    "y_targ = foil['y'].y\n",
    "y_pred = model.predict(np.expand_dims(X_pred,axis=0))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(foil['y'].x, foil['y'].y), plt.plot(foil['y'].x, y_pred[0,])#, plt.plot([0,0],[-0.3,0.3])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_generator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in model.layers:\n",
    "#     print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[2].bias_initializer=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet = model(64, 64, optimizer=Adam(learning_rate=lr), reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred.shape, y_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred[:,0], y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
